{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277c42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 08:07:46.332150: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-07 08:07:46.332178: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-07 08:07:49.915734: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-07 08:07:49.915757: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-07 08:07:49.915772: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-07 08:07:49.915992: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-07 08:07:49.928205: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight', 'transformer.h.11.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835  7177]], shape=(1, 2), dtype=int32)\n",
      "[17835, 7177, 6995, 16098, 7281, 9885, 13358, 10010, 6947, 7399, 7220, 9022, 6866, 19588, 9077, 18337, 17955, 16913, 13485, 8146, 8196, 9265, 7162, 9018, 7895, 10936, 9034, 8325, 9148, 45887, 9402, 19495, 24117, 8137, 12904, 10590, 11698, 32937, 9351, 7470, 19325, 8702, 11768, 9129, 10542, 19561, 7788, 15709, 9782, 11649, 13023, 9337, 15092, 8092, 9620, 22375, 9076, 9038, 9863, 10578, 15605, 8263, 35453, 11718, 21319, 7532, 15378, 10401, 50997, 9277, 19635, 8075, 11594, 9199, 9929, 6824, 13675, 30903, 11114, 9355, 12517, 43242, 13203, 9134, 18607, 9362, 39376, 43056, 13768, 28569, 24488, 406, 9316, 32010, 23753, 7991, 15525, 37767, 10070, 7235, 10917, 24454, 11387, 35187, 20337, 31994, 9046, 7890, 25226, 9272, 46588, 14485, 9172, 7587, 13486, 9723, 681, 9661, 16691, 8, 12199, 8711, 10033, 13805, 21734, 9563, 19367, 13386]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'예수님께 기도드리고 싶습니다.\\n그런데 그게 무슨 소용이 있겠습니까?\\n이제 저는 이십 년 전쯤부터 제가 하나님의 말씀을 듣고 있습니다.\\n하나님은 우리를 구원하시는 분입니다.\\n그래서 우리가 지금 어떻게 해야 할까요?\\n우리가 무엇을 원하고 어떤 일을 원하는지 알아야 합니다.\\n그러면 우리는 왜 살아야 하는가?\\n왜 사는 것이 필요한가요?\\n우리는 무엇 때문에 살고 있는가?\\n우리에게 주어진 것은 무엇인가?\\n그리고 우리의 삶은 무엇인가?라는 질문을 던져야 됩니다.\\n저는 오늘도 이렇게 질문합니다.\\n예수님이 우리에게 주신 메시지는 무엇일까요?\\n바로 ‘믿음의 힘’이었습니다.</d> 지난해 12월 31일 오후 서울 종로구 세종'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPTv2 모델 사용\n",
    "# TFGPT2LMHeadModel.from_pretrained('GPT 모델 이름') : 두개의 문장이 이어지는 문장인지를 판단하는 GPT 모델 로드\n",
    "# AutoTokenizer.from_pretrained('GPT 모델 이름') : 위 로드된 모델이 학습되었을 당시에 사용된 토크나이저를 로드\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "output = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # avoiding repeated tokens (higher value means more diverse output)\n",
    "                        use_cache=True) # enables or disables the use of the model's internal cache (repetitive output)\n",
    "\n",
    "output_ids = output.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5335b500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['께', '께서', '과', ',', '께서는']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력문장의 다음 단어로 가장 확률이 높은 단어 Top 5 예측\n",
    "output = model(input_ids)\n",
    "top5 = tf.math.top_k(output.logits[0, -1], k=5)\n",
    "tokenizer.convert_ids_to_tokens(top5.indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b082e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install tensorflow==2.7.0\\npip install transformers==4.21.0\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upgrade transformers and datasets to latest versions\n",
    "'''\n",
    "pip install tensorflow==2.7.0\n",
    "pip install transformers==4.21.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19a3d4a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight', 'transformer.h.11.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 27695), dtype=int32, numpy=array([[ 9724,  7492,  7953, ...,  8705, 32240,  9051]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27695), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Step 0 Loss 3.850346565246582\n",
      "Time duration(in seconds): 11.203914273006376\n"
     ]
    }
   ],
   "source": [
    "# GPTv2 모델 파인튜닝 & 저장\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the text data\n",
    "with open('bible_john_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Instantiate the GPT-2 model\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "print(tokenized_text)\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # Usually 1024 for GPT-2\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 16\n",
    "learning_rate = 3e-5\n",
    "epochs = 1\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(f'./output/gpt2-finetuned-epoch-{epoch+1}')\n",
    "    tokenizer.save_pretrained(f'./output/gpt2-finetuned-epoch-{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d5fc39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:  예수님\n",
      "Generated Summary:  예수님께 감사드린다”며 “예수님께서 우리를 사랑하셨기 때문에 우리도 사랑할 수 있었다”고 고백했다.\n",
      "이어 “우리가 예수님을 사랑하지 않았기 때문에 우리가 우리\n"
     ]
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 요약\n",
    "# Note that the model_path variable should be set to the path where the saved model is stored on your machine.\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-1\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Generate summaries\n",
    "max_length = 40\n",
    "num_beams = 4\n",
    "input_text = \"예수님\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "output_ids = model.generate(input_ids, max_length=max_length, num_beams=num_beams, no_repeat_ngram_size=2, early_stopping=True)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Input Text: \", input_text)\n",
    "print(\"Generated Summary: \", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03b1dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835  7177]], shape=(1, 2), dtype=int32)\n",
      "[17835, 7177, 6995, 16098, 7281, 9885, 13358, 8017, 10010, 6947, 7399, 7220, 9022, 6866, 44224, 14782, 7965, 7172, 7532, 17582, 10401, 14927, 9366, 33866, 9535, 7422, 15433, 42138, 8006, 13675, 22507, 15570, 9564, 9032, 9758, 7916, 12583, 43913, 9394, 9863, 10229, 45023, 19588, 11271, 9651, 19825, 18663, 9176, 12102, 11649, 12341, 15597, 24692, 9207, 9548, 9290, 9054, 38461, 6889, 9108, 11993, 13166, 19520, 13997, 9339, 9348, 9094, 7621, 7415, 7258, 8263, 10171, 16913, 13485, 10972, 10185, 9179, 9661, 34693, 43056, 9063, 12333, 32434, 11106, 18961, 8075, 7080, 9511, 11777, 45937, 14686, 13229, 13953, 24022, 9237, 34487, 19561, 7788, 9258, 10152, 13548, 11382, 7489, 23532, 16522, 8718, 13083, 6969, 10078, 21319, 23775, 9871, 9306, 14208, 7162, 11190, 19730, 7235, 14611, 9801, 15538, 9745, 23236, 38760, 22944, 8704, 9199, 9554]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'예수님께 기도드리고 싶었습니다.\\n그런데 그게 아니었어요.\\n아니면 내가 왜 그렇게 많은 기도를 드렸는지 모르겠어요?\\n내가 얼마나 많이 기도로써 왔는지, 그리고 어떤 사람이 내게 무슨 말을 했는지는 모르지만 나는 어떻게 해서든지 나를 위해 모든 것을 다 바쳤고 또 그것을 통해서 나의 삶을 살도록 내버려두지 않았습니까?\\n나는 그런 사람이었죠.\\n그리고 나에게는 아무것도 없었지요.\\n왜냐하면 그것은 나에게 아무런 도움이 되지 않았기 때문이지요.\\n그래서 그는 자기 자신을 위해서만 자기를 희생했으니까요.\\n그러니까 그가 다른 사람들에게는 아무 도움도 주지 않고 오직 자신의 삶에 대해서만 헌신한 것이 아니라'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 생성\n",
    "# In this example, we first load the tokenizer and fine-tuned model using their respective paths.\n",
    "# Then we set the generation parameters: max_length controls the maximum length of the generated text,\n",
    "# temperature controls the randomness of the text (higher temperature leads to more random and creative output),\n",
    "# and num_beams controls the number of beams used in beam search for text generation.\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-1\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set the generation parameters\n",
    "max_length = 100\n",
    "temperature = 0.5\n",
    "num_beams = 4\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "output = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # avoiding repeated tokens (higher value means more diverse output)\n",
    "                        use_cache=True) # enables or disables the use of the model's internal cache (repetitive output)\n",
    "\n",
    "output_ids = output.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "tokenizer.decode(output_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
