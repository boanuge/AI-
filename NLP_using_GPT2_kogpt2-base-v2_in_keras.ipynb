{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "277c42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'lm_head.weight', 'transformer.h.11.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835  7177]], shape=(1, 2), dtype=int32)\n",
      "[17835, 7177, 6995, 16098, 7281, 9885, 13358, 10010, 6947, 7399, 7220, 9022, 6866, 19588, 9077, 18337, 17955, 16913, 13485, 8146, 8196, 9265, 7162, 9018, 7895, 10936, 9034, 8325, 9148, 45887, 9402, 19495, 24117, 8137, 12904, 10590, 11698, 32937, 9351, 7470, 19325, 8702, 11768, 9129, 10542, 19561, 7788, 15709, 9782, 11649, 13023, 9337, 15092, 8092, 9620, 22375, 9076, 9038, 9863, 10578, 15605, 8263, 35453, 11718, 21319, 7532, 15378, 10401, 50997, 9277, 19635, 8075, 11594, 9199, 9929, 6824, 13675, 30903, 11114, 9355, 12517, 43242, 13203, 9134, 18607, 9362, 39376, 43056, 13768, 28569, 24488, 406, 9316, 32010, 23753, 7991, 15525, 37767, 10070, 7235, 10917, 24454, 11387, 35187, 20337, 31994, 9046, 7890, 25226, 9272, 46588, 14485, 9172, 7587, 13486, 9723, 681, 9661, 16691, 8, 12199, 8711, 10033, 13805, 21734, 9563, 19367, 13386]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'예수님께 기도드리고 싶습니다.\\n그런데 그게 무슨 소용이 있겠습니까?\\n이제 저는 이십 년 전쯤부터 제가 하나님의 말씀을 듣고 있습니다.\\n하나님은 우리를 구원하시는 분입니다.\\n그래서 우리가 지금 어떻게 해야 할까요?\\n우리가 무엇을 원하고 어떤 일을 원하는지 알아야 합니다.\\n그러면 우리는 왜 살아야 하는가?\\n왜 사는 것이 필요한가요?\\n우리는 무엇 때문에 살고 있는가?\\n우리에게 주어진 것은 무엇인가?\\n그리고 우리의 삶은 무엇인가?라는 질문을 던져야 됩니다.\\n저는 오늘도 이렇게 질문합니다.\\n예수님이 우리에게 주신 메시지는 무엇일까요?\\n바로 ‘믿음의 힘’이었습니다.</d> 지난해 12월 31일 오후 서울 종로구 세종'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPTv2 모델 사용\n",
    "# TFGPT2LMHeadModel.from_pretrained('GPT 모델 이름') : 두개의 문장이 이어지는 문장인지를 판단하는 GPT 모델 로드\n",
    "# AutoTokenizer.from_pretrained('GPT 모델 이름') : 위 로드된 모델이 학습되었을 당시에 사용된 토크나이저를 로드\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "output = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # avoiding repeated tokens (higher value means more diverse output)\n",
    "                        use_cache=True) # enables or disables the use of the model's internal cache (repetitive output)\n",
    "\n",
    "output_ids = output.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력문장의 다음 단어로 가장 확률이 높은 단어 Top 5 예측\n",
    "output = model(input_ids)\n",
    "top5 = tf.math.top_k(output.logits[0, -1], k=5)\n",
    "tokenizer.convert_ids_to_tokens(top5.indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b082e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upgrade transformers and datasets to latest versions\n",
    "'''\n",
    "pip install tensorflow==2.7.0\n",
    "pip install transformers==4.21.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a3d4a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 22:02:30.539018: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-07 22:02:30.539044: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-07 22:02:34.136619: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-07 22:02:34.136642: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-07 22:02:34.136657: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-07 22:02:34.136860: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-07 22:02:34.149382: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.3.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 27695), dtype=int32, numpy=array([[ 9724,  7492,  7953, ...,  8705, 32240,  9051]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27695), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5abffcda60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5abffcda60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Step 0 Loss 3.841675043106079\n",
      "Time duration(in seconds): 15.190023487026338\n"
     ]
    }
   ],
   "source": [
    "# GPTv2 모델 파인튜닝 & 저장\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the text data\n",
    "with open('bible_john_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Instantiate the GPT-2 model\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "print(tokenized_text)\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # Usually 1024 for GPT-2\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 16\n",
    "learning_rate = 3e-5\n",
    "epochs = 1\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(f'./output/gpt2-finetuned-epoch-{epoch+1}')\n",
    "    tokenizer.save_pretrained(f'./output/gpt2-finetuned-epoch-{epoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d5fc39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:  예수님\n",
      "Generated Summary:  예수님께 감사드립니다.\"\n",
      "\"감사합니다, 목사님.\"\n",
      "\"목사님, 저는 저를 사랑하고 있습니다.\"\n",
      "그녀는 고개를 끄덕였다.\n",
      "\"그런데 왜 저\n"
     ]
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 요약\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-1\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Generate summaries\n",
    "max_length = 40\n",
    "num_beams = 4\n",
    "input_text = \"예수님\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "output_ids = model.generate(input_ids, max_length=max_length, num_beams=num_beams, no_repeat_ngram_size=2, early_stopping=True)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Input Text: \", input_text)\n",
    "print(\"Generated Summary: \", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03b1dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835  7177]], shape=(1, 2), dtype=int32)\n",
      "[17835, 7177, 6995, 16098, 7281, 9885, 13358, 8017, 10010, 6947, 7399, 7220, 9022, 13041, 13080, 7816, 8137, 9068, 12102, 9018, 15406, 9080, 9548, 10574, 9054, 9393, 14486, 9290, 12487, 9414, 14782, 10972, 12858, 17582, 10909, 33561, 7489, 9181, 12306, 14085, 387, 9455, 9351, 7470, 29543, 9179, 9383, 9658, 8718, 18961, 47637, 11791, 9432, 44235, 19826, 22507, 11273, 9846, 10948, 15378, 9402, 19495, 10156, 9445, 10687, 15562, 9481, 34693, 43056, 14121, 24692, 25203, 11611, 13885, 9362, 10021, 9063, 9415, 9661, 31204, 10078, 21319, 9433, 15709, 24736, 9173, 31011, 9927, 13768, 14145, 6958, 9355, 9258, 19520, 12683, 9316, 9572, 9237, 19747, 9135, 9685, 17155, 7978, 6872, 8263, 13675, 8146, 8196, 9148, 9094, 39576, 45937, 10166, 14955, 16085, 18789, 9075, 9294, 11283, 9871, 45023, 7788, 14269, 9199, 9554, 13076, 13300, 13680]\n",
      "예수님께 기도드리고 싶었습니다.\n",
      "그런데 그분이 돌아가셨을 때 나는 이 땅에 있는 모든 사람들이 다 죽었다는 것을 알고 있었어요.\n",
      "나는 그때 내가 죽은 줄로만 알았던 사람들, 즉 우리를 죽인 사람이라고 생각했지요.\n",
      "그러나 그들은 모두 죽었답니다.\n",
      "내가 살아 있을 때, 우리는 하나님의 아들이라는 사실을 알게 되었죠.\n",
      "그리고 그들이 나를 죽이려고 했던 것은 바로 나 자신이었거든요.\n",
      "그러므로 우리가 죽는다는 것은, 곧 우리의 아들이기 때문에 그는 나의 아버지라는 사실 때문이었다고 말해야 옳았겠지요?\n",
      "이제부터 내 아들은 나에게 있어서 아버지의 아들로 태어났고, 그의 아버지는 그가 내게서 태어난 것이 아니라 그에게 와서 태어나\n",
      "Time duration(in seconds): 30.32901354995556\n",
      "예수님께 기도드리고 싶었습니다.\n",
      "그런데 그분이 돌아가셨을 때 나는 이 땅에 있는 모든 사람들이 다 죽었다는 것을 알고 있었어요.\n",
      "나는 그때 내가 죽은 줄로만 알았던 사람들, 즉 우리를 죽인 사람이라고 생각했지요.\n",
      "그러나 그들은 모두 죽었답니다.\n",
      "내가 살아 있을 때, 우리는 하나님의 아들이라는 사실을 알게 되었죠.\n",
      "그리고 그들이 나를 죽이려고 했던 것은 바로 나 자신이었거든요.\n",
      "그러므로 우리가 죽는다는 것은, 곧 우리의 아들이기 때문에 그는 나의 아버지라는 사실 때문이었다고 말해야 옳았겠지요?\n",
      "이제부터 내 아들은 나에게 있어서 아버지의 아들로 태어났고, 그의 아버지는 그가 내게서 태어난 것이 아니라 그에게 와서 태어나\n",
      "Time duration(in seconds): 30.493720119993668\n",
      "예수님께, 그분의 얼굴을 보지도 말고, 내가 우리를 대신하여, 내 이름을 부르고, 나의 친구에게 가서 이야기를 나누며, 또 우리에게 자신의 말을 할 기회를 허락하신 다음, 바로 그날이 되면, 그곳으로 가야 한다는 사실을 기억하시고 이 때에 함께 가는 것이 당신을 더 현명하게 만드는 길임을 알게 될 거예요.\n",
      "그 다음에는 반드시 다른 곳으로 가려고 하고.\n",
      "다른 사람이 누구냐는 말도 잊지 않고, 단지 그를 위해서 나를 만날 수 있겠어요.\n",
      "그는 언제나 그곳에 있다고 확신하고 있어요.\n",
      "그리고 나는 어떤 장소에도 갈 수는 없지만, 그는 거기서도 그가 온다는 생각만으로 아주 멀리까지 와 있을 뿐이에요.\n",
      "그러므로 그것은 우리가 그에게만 있는 것이라고 생각하고, 그들이\n",
      "Time duration(in seconds): 30.524865015991963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ngenerated_text = \"\"\\nwhile True:\\n    # Generate one sequence of text\\n    output = model.generate(input_ids, max_length=40, repetition_penalty=2.0)\\n    sequence = tokenizer.decode(output[0], skip_special_tokens=True)\\n    generated_text += sequence.strip()\\n    if generated_text.endswith((\".\", \"!\", \"?\")):\\n        break\\nreturn generated_text\\nprint(\"A complete sentence has been generated :\", generated_text)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 생성\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-1\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "start = default_timer()\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "generated_ids = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # 1.0 indicates no penalty for repeating tokens, up to the 2.0\n",
    "                        num_return_sequences=1, # the number of independent sequences to generate for each prompt\n",
    "                        early_stopping=True, # stops generating a sentence before max_length working with eos_token_id\n",
    "                        use_cache=True) # enables or disables the use of the model's internal cache (repetitive output)\n",
    "\n",
    "output_ids = generated_ids.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "start = default_timer()\n",
    "# 가장 높은 확률 순으로 한 문장만 완성될 때까지만 생성\n",
    "generated_ids = model.generate(input_ids, max_length=128, repetition_penalty=2.0, early_stopping=True, eos_token_id=tokenizer.eos_token_id)\n",
    "output_ids = generated_ids.numpy().tolist()[0]\n",
    "decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "start = default_timer()\n",
    "# 높은 확률(44위)내 랜덤으로 한 문장만 완성될 때까지만 생성 : 부자연스러운 문장이 될 수 있음\n",
    "generated_ids = model.generate(input_ids, max_length=128, repetition_penalty=2.0, do_sample=True, top_k=44, early_stopping=True, eos_token_id=tokenizer.eos_token_id)\n",
    "output_ids = generated_ids.numpy().tolist()[0]\n",
    "decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# Stop generating text if the generated text is a complete sentence\n",
    "'''\n",
    "generated_text = \"\"\n",
    "while True:\n",
    "    # Generate one sequence of text\n",
    "    output = model.generate(input_ids, max_length=40, repetition_penalty=2.0)\n",
    "    sequence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_text += sequence.strip()\n",
    "    if generated_text.endswith((\".\", \"!\", \"?\")):\n",
    "        break\n",
    "return generated_text\n",
    "print(\"A complete sentence has been generated :\", generated_text)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
