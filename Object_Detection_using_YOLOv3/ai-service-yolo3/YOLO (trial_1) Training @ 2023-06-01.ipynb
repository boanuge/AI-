{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1b48d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 09:37:16.662823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 09:37:16.794765: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-13 09:37:16.832386: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-13 09:37:17.710520: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-13 09:37:17.710604: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-13 09:37:17.710612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-13 09:37:18.893710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:18.935331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:18.996842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:18.999719: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 09:37:19.000224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:19.002234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:19.004216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:19.875920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:19.877356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:19.878531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-13 09:37:19.879649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12288 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocation( 12288 ) # of Physical GPU( 1 ) # of Logical GPU( 1 )\n",
      " @ CDT(2023-06-13T09:37:19.978990)\n",
      "Create YOLOv3 model with 9 anchors and 1 classes.\n",
      "WARNING:tensorflow:Skipping loading weights for layer #249 (named conv2d_58) due to mismatch in shape for weight conv2d_58/kernel:0. Weight expects shape (1, 1, 1024, 18). Received saved weight with shape (255, 1024, 1, 1)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #249 (named conv2d_58) due to mismatch in shape for weight conv2d_58/bias:0. Weight expects shape (18,). Received saved weight with shape (255,)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #250 (named conv2d_66) due to mismatch in shape for weight conv2d_66/kernel:0. Weight expects shape (1, 1, 512, 18). Received saved weight with shape (255, 512, 1, 1)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #250 (named conv2d_66) due to mismatch in shape for weight conv2d_66/bias:0. Weight expects shape (18,). Received saved weight with shape (255,)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named conv2d_74) due to mismatch in shape for weight conv2d_74/kernel:0. Weight expects shape (1, 1, 256, 18). Received saved weight with shape (255, 256, 1, 1)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named conv2d_74) due to mismatch in shape for weight conv2d_74/bias:0. Weight expects shape (18,). Received saved weight with shape (255,)\n",
      "Load weights yolo3_weights_via_MSCOCO.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "Trainable parameters:\n",
      "tf.Tensor(32310, shape=(), dtype=int32)\n",
      "Non-trainable parameters:\n",
      "tf.Tensor(61544032, shape=(), dtype=int32)\n",
      "Train on 20875 samples, val on 2319 samples, with batch size 32.\n",
      "Train the model @ CDT(2023-06-13T09:37:23.757469)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 09:37:38.777384: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700\n",
      "2023-06-13 09:37:39.635291: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652/652 [==============================] - 1710s 3s/step - loss: 206.0760 - val_loss: 26.7741 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "652/652 [==============================] - 1687s 3s/step - loss: 22.5646 - val_loss: 20.0711 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "652/652 [==============================] - 1686s 3s/step - loss: 19.0723 - val_loss: 18.3259 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "652/652 [==============================] - 1687s 3s/step - loss: 17.8813 - val_loss: 17.3398 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "652/652 [==============================] - 1681s 3s/step - loss: 17.2337 - val_loss: 16.9966 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "652/652 [==============================] - 1682s 3s/step - loss: 16.8085 - val_loss: 16.6155 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "652/652 [==============================] - 1680s 3s/step - loss: 16.5337 - val_loss: 16.3863 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "652/652 [==============================] - 1680s 3s/step - loss: 16.2803 - val_loss: 16.0533 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "652/652 [==============================] - 1676s 3s/step - loss: 16.1356 - val_loss: 16.0742 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "652/652 [==============================] - 1673s 3s/step - loss: 15.9833 - val_loss: 15.9739 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "652/652 [==============================] - 1677s 3s/step - loss: 15.8669 - val_loss: 15.7582 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "652/652 [==============================] - 1690s 3s/step - loss: 15.7961 - val_loss: 15.6586 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "652/652 [==============================] - 1682s 3s/step - loss: 15.7343 - val_loss: 15.6163 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "652/652 [==============================] - 1679s 3s/step - loss: 15.6540 - val_loss: 15.6051 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "652/652 [==============================] - 1679s 3s/step - loss: 15.6299 - val_loss: 15.5592 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "652/652 [==============================] - 1681s 3s/step - loss: 15.6188 - val_loss: 15.5049 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "652/652 [==============================] - 1690s 3s/step - loss: 15.5871 - val_loss: 15.5313 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "652/652 [==============================] - 1682s 3s/step - loss: 15.5707 - val_loss: 15.4743 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "652/652 [==============================] - 1675s 3s/step - loss: 15.5361 - val_loss: 15.4735 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "652/652 [==============================] - 1674s 3s/step - loss: 15.4949 - val_loss: 15.3925 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "652/652 [==============================] - 1673s 3s/step - loss: 15.5178 - val_loss: 15.5231 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "652/652 [==============================] - 1681s 3s/step - loss: 15.4840 - val_loss: 15.3801 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "652/652 [==============================] - 1681s 3s/step - loss: 15.4819 - val_loss: 15.3598 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "652/652 [==============================] - 1666s 3s/step - loss: 15.5047 - val_loss: 15.4816 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "652/652 [==============================] - 1679s 3s/step - loss: 15.4661 - val_loss: 15.3612 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "652/652 [==============================] - ETA: 0s - loss: 15.4888\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "652/652 [==============================] - 1665s 3s/step - loss: 15.4888 - val_loss: 15.4523 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "652/652 [==============================] - 1679s 3s/step - loss: 15.3047 - val_loss: 15.2355 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "652/652 [==============================] - 1670s 3s/step - loss: 15.2793 - val_loss: 15.2689 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "652/652 [==============================] - 1671s 3s/step - loss: 15.2528 - val_loss: 15.1966 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "652/652 [==============================] - 1652s 3s/step - loss: 15.2521 - val_loss: 15.2431 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "652/652 [==============================] - 1662s 3s/step - loss: 15.2475 - val_loss: 15.1934 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "652/652 [==============================] - 1672s 3s/step - loss: 15.2587 - val_loss: 15.3082 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "652/652 [==============================] - 1681s 3s/step - loss: 15.2461 - val_loss: 15.1710 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "652/652 [==============================] - 1663s 3s/step - loss: 15.2770 - val_loss: 15.2359 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "652/652 [==============================] - 1668s 3s/step - loss: 15.2423 - val_loss: 15.1871 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "652/652 [==============================] - ETA: 0s - loss: 15.2621\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "652/652 [==============================] - 1665s 3s/step - loss: 15.2621 - val_loss: 15.2336 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "652/652 [==============================] - 1662s 3s/step - loss: 15.2087 - val_loss: 15.1025 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "652/652 [==============================] - 1664s 3s/step - loss: 15.2278 - val_loss: 15.1865 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "652/652 [==============================] - 1663s 3s/step - loss: 15.2435 - val_loss: 15.1720 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "652/652 [==============================] - ETA: 0s - loss: 15.2379\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "652/652 [==============================] - 1666s 3s/step - loss: 15.2379 - val_loss: 15.1313 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "652/652 [==============================] - 1660s 3s/step - loss: 15.2161 - val_loss: 15.1578 - lr: 1.0000e-06\n",
      "Epoch 42/100\n",
      "652/652 [==============================] - 1665s 3s/step - loss: 15.2473 - val_loss: 15.1697 - lr: 1.0000e-06\n",
      "Epoch 43/100\n",
      "652/652 [==============================] - ETA: 0s - loss: 15.2361\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "652/652 [==============================] - 1672s 3s/step - loss: 15.2361 - val_loss: 15.1933 - lr: 1.0000e-06\n",
      "Epoch 44/100\n",
      "652/652 [==============================] - 1678s 3s/step - loss: 15.2242 - val_loss: 15.1395 - lr: 1.0000e-07\n",
      "Epoch 45/100\n",
      "652/652 [==============================] - 1667s 3s/step - loss: 15.2394 - val_loss: 15.1709 - lr: 1.0000e-07\n",
      "Epoch 46/100\n",
      "652/652 [==============================] - ETA: 0s - loss: 15.2261\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "652/652 [==============================] - 1672s 3s/step - loss: 15.2261 - val_loss: 15.2669 - lr: 1.0000e-07\n",
      "Epoch 47/100\n",
      "652/652 [==============================] - 1662s 3s/step - loss: 15.2403 - val_loss: 15.2007 - lr: 1.0000e-08\n",
      "Epoch 47: early stopping\n",
      "Save the model @ CDT(2023-06-14T07:29:18.699615)\n",
      " @ CDT(2023-06-14T07:29:19.435069)\n"
     ]
    }
   ],
   "source": [
    "# Only CPU 약 하루 걸림 @ 테스트 서버\n",
    "\n",
    "model_path = 'yolo3_trained_trial_1_@_20230601.h5'\n",
    "model_path_inference = 'yolo3_trained_trial_1_@_20230601_inf.h5'\n",
    "\n",
    "annotation_path = './yolo_train.txt'\n",
    "classes_path = './yolo_classes.txt'\n",
    "anchors_path = './yolo_anchors.txt'\n",
    "\n",
    "freeze_body = 2 # 1 = \"Freeze the first 185 layers of total 252 layers\", 2 = Train top 3 layers, 3 = Train top 9 layers\n",
    "batch_size = 32\n",
    "learning_rate = 0.001 # 0.001 = 1e-3\n",
    "random_aug = True\n",
    "\n",
    "import os # Protocol Buffers to use Python rather than C++\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "## GPU에 할당(사용)되는 메모리 크기 제한\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpu_memory_limit=1024*12 # only allocate 12GB of memory on the gpus[0], i.e. first GPU\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=gpu_memory_limit)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(\"GPU memory allocation(\",gpu_memory_limit,\")\",\n",
    "          \"# of Physical GPU(\",len(gpus),\") # of Logical GPU(\",len(logical_gpus),\")\")\n",
    "  except RuntimeError as e:\n",
    "    print(e) # Virtual devices must be set before GPUs have been initialized\n",
    "else:\n",
    "   print(\"There is no GPU to use.\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from yolo3_model_for_training import preprocess_true_boxes, yolo_body, yolo_loss, get_random_data\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True,\n",
    "freeze_body=freeze_body, weights_path='yolo3_weights_via_MSCOCO.h5'): # Full filtering\n",
    "#freeze_body=2, weights_path='yolo3_weights_via_MSCOCO.h5'):\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2, 3]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3, len(model_body.layers)-9)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=random_aug)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)\n",
    "\n",
    "input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "model = create_model(input_shape, anchors, num_classes)\n",
    "\n",
    "val_split = 0.1\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val\n",
    "\n",
    "# Adjust learning rate (use default Adam) to avoid \"Gradient Explosion\"\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss={ # Removing learning_rate because of gradient explosion\n",
    "    'yolo_loss': lambda y_true, y_pred: y_pred}) # use custom yolo_loss Lambda layer.\n",
    "\n",
    "# Print trainable and non-trainable parameters\n",
    "print(\"Trainable parameters:\")\n",
    "print(tf.reduce_sum([tf.reduce_prod(w.shape) for w in model.trainable_weights]))\n",
    "print(\"Non-trainable parameters:\")\n",
    "print(tf.reduce_sum([tf.reduce_prod(w.shape) for w in model.non_trainable_weights]))\n",
    "\n",
    "print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "\n",
    "print_current_datetime(\"Train the model\")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "        steps_per_epoch=max(1, num_train//batch_size),\n",
    "        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "        validation_steps=max(1, num_val//batch_size),\n",
    "        epochs=100,\n",
    "        initial_epoch=0,\n",
    "        verbose=1,\n",
    "        callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "print_current_datetime(\"Save the model\")\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "print_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a167b77a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yolo_body' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\AI\\ai-service-image-gcooter @ 2023-06-01 (학습용)\\YOLO Gcooter (trial_1) Training @ 2023-06-01 (testing).ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/AI/ai-service-image-gcooter%20%40%202023-06-01%20%28%ED%95%99%EC%8A%B5%EC%9A%A9%29/YOLO%20Gcooter%20%28trial_1%29%20Training%20%40%202023-06-01%20%28testing%29.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# To prevent yolo_loss() NameError: name 'K' is not defined\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/AI/ai-service-image-gcooter%20%40%202023-06-01%20%28%ED%95%99%EC%8A%B5%EC%9A%A9%29/YOLO%20Gcooter%20%28trial_1%29%20Training%20%40%202023-06-01%20%28testing%29.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m inference_model \u001b[39m=\u001b[39m yolo_body(Input(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39m3\u001b[39m)), \u001b[39mlen\u001b[39m(anchors)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m, num_classes)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/AI/ai-service-image-gcooter%20%40%202023-06-01%20%28%ED%95%99%EC%8A%B5%EC%9A%A9%29/YOLO%20Gcooter%20%28trial_1%29%20Training%20%40%202023-06-01%20%28testing%29.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m inference_model\u001b[39m.\u001b[39mload_weights(model_path, by_name\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, skip_mismatch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/AI/ai-service-image-gcooter%20%40%202023-06-01%20%28%ED%95%99%EC%8A%B5%EC%9A%A9%29/YOLO%20Gcooter%20%28trial_1%29%20Training%20%40%202023-06-01%20%28testing%29.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m inference_model\u001b[39m.\u001b[39msave(model_path_inference)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yolo_body' is not defined"
     ]
    }
   ],
   "source": [
    "# To prevent yolo_loss() NameError: name 'K' is not defined\n",
    "inference_model = yolo_body(Input(shape=(None, None, 3)), len(anchors)//3, num_classes)\n",
    "inference_model.load_weights(model_path, by_name=True, skip_mismatch=True)\n",
    "inference_model.save(model_path_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec85b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss', 'lr'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff25d9f1fa0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b+', label='Validation val_loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
