{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1b48d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no GPU to use.\n",
      " @ CDT(2023-06-15T09:44:25.844550)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = create_model(input_shape, anchors, num_classes)\\n\\nval_split = 0.1\\nwith open(annotation_path) as f:\\n    lines = f.readlines()\\nnp.random.seed(10101)\\nnp.random.shuffle(lines)\\nnp.random.seed(None)\\nnum_val = int(len(lines)*val_split)\\nnum_train = len(lines) - num_val\\n\\n# Adjust learning rate (use default Adam) to avoid \"Gradient Explosion\"\\nmodel.compile(optimizer=Adam(learning_rate=learning_rate), loss={ # Removing learning_rate because of gradient explosion\\n    \\'yolo_loss\\': lambda y_true, y_pred: y_pred}) # use custom yolo_loss Lambda layer.\\n\\n# Print trainable and non-trainable parameters\\nprint(\"Trainable parameters:\")\\nprint(tf.reduce_sum([tf.reduce_prod(w.shape) for w in model.trainable_weights]))\\nprint(\"Non-trainable parameters:\")\\nprint(tf.reduce_sum([tf.reduce_prod(w.shape) for w in model.non_trainable_weights]))\\n\\nprint(\\'Train on {} samples, val on {} samples, with batch size {}.\\'.format(num_train, num_val, batch_size))\\n\\nprint_current_datetime(\"Train the model\")\\n\\nreduce_lr = ReduceLROnPlateau(monitor=\\'val_loss\\', factor=0.1, patience=3, verbose=1)\\nearly_stopping = EarlyStopping(monitor=\\'val_loss\\', min_delta=0, patience=10, verbose=1)\\n\\nhistory = model.fit(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\\n        steps_per_epoch=max(1, num_train//batch_size),\\n        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\\n        validation_steps=max(1, num_val//batch_size),\\n        epochs=100,\\n        initial_epoch=0,\\n        verbose=1,\\n        callbacks=[reduce_lr, early_stopping])\\n\\nprint_current_datetime(\"Save the model\")\\n\\nmodel.save(model_path)\\n\\nprint_current_datetime()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'yolo3_trained_trial_1_@_20230601.h5'\n",
    "model_path_inference = 'yolo3_trained_trial_1_@_20230601_inf.h5'\n",
    "\n",
    "annotation_path = './yolo_train.txt'\n",
    "classes_path = './yolo_classes.txt'\n",
    "anchors_path = './yolo_anchors.txt'\n",
    "\n",
    "freeze_body = 2 # 1 = \"Freeze the first 185 layers of total 252 layers\", 2 = Train top 3 layers, 3 = Train top 9 layers\n",
    "batch_size = 32\n",
    "learning_rate = 0.001 # 0.001 = 1e-3\n",
    "random_aug = True\n",
    "\n",
    "import os # Protocol Buffers to use Python rather than C++\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "## GPU에 할당(사용)되는 메모리 크기 제한\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpu_memory_limit=1024*12 # only allocate 12GB of memory on the gpus[0], i.e. first GPU\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=gpu_memory_limit)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(\"GPU memory allocation(\",gpu_memory_limit,\")\",\n",
    "          \"# of Physical GPU(\",len(gpus),\") # of Logical GPU(\",len(logical_gpus),\")\")\n",
    "  except RuntimeError as e:\n",
    "    print(e) # Virtual devices must be set before GPUs have been initialized\n",
    "else:\n",
    "   print(\"There is no GPU to use.\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from yolo3_model_for_training import preprocess_true_boxes, yolo_body, yolo_loss, get_random_data\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True,\n",
    "freeze_body=freeze_body, weights_path='yolo3_weights_via_MSCOCO.h5'): # Full filtering\n",
    "#freeze_body=2, weights_path='yolo3_weights_via_MSCOCO.h5'):\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2, 3]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3, len(model_body.layers)-9)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=random_aug)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)\n",
    "\n",
    "input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "# To prevent yolo_loss() NameError: name 'K' is not defined\n",
    "inference_model = yolo_body(Input(shape=(None, None, 3)), len(anchors)//3, num_classes)\n",
    "#inference_model.load_weights(model_path, by_name=True, skip_mismatch=True)\n",
    "inference_model.load_weights(model_path)\n",
    "inference_model.save(model_path_inference)\n",
    "'''\n",
    "model = create_model(input_shape, anchors, num_classes)\n",
    "\n",
    "val_split = 0.1\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val\n",
    "\n",
    "# Adjust learning rate (use default Adam) to avoid \"Gradient Explosion\"\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss={ # Removing learning_rate because of gradient explosion\n",
    "    'yolo_loss': lambda y_true, y_pred: y_pred}) # use custom yolo_loss Lambda layer.\n",
    "\n",
    "# Print trainable and non-trainable parameters\n",
    "print(\"Trainable parameters:\")\n",
    "print(tf.reduce_sum([tf.reduce_prod(w.shape) for w in model.trainable_weights]))\n",
    "print(\"Non-trainable parameters:\")\n",
    "print(tf.reduce_sum([tf.reduce_prod(w.shape) for w in model.non_trainable_weights]))\n",
    "\n",
    "print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "\n",
    "print_current_datetime(\"Train the model\")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "        steps_per_epoch=max(1, num_train//batch_size),\n",
    "        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "        validation_steps=max(1, num_val//batch_size),\n",
    "        epochs=100,\n",
    "        initial_epoch=0,\n",
    "        verbose=1,\n",
    "        callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "print_current_datetime(\"Save the model\")\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "print_current_datetime()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
