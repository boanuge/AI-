{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61192c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 00:06:59.315659: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 00:06:59.467601: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 00:06:59.467621: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-24 00:07:00.469239: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 00:07:00.469296: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 00:07:00.469302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 사용\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a16ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 맞게 설정됐는지 확인하는 코드\n",
    "'''\n",
    "n_devices = torch.cuda.device_count()\n",
    "print(n_devices)\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d029a3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n",
      "                         Q                     A  label\n",
      "10186   썸 타는 노래 듣는 중인데 좋아.         썸 탈 때 듣기 딱이죠.      2\n",
      "8544   헤어지고서 10개월동안 4번 봤네.               많이 봤네요.      1\n",
      "3345            오늘은 또 뭐 입냐         아침마다 하는 고민이죠.      0\n",
      "8096           제 친구가 그러더라구  친구보다 자신의 마음에 귀기울이세요.      1\n",
      "3617             은퇴후가 걱정이야           인생이 꽤 기니까요.      0\n"
     ]
    }
   ],
   "source": [
    "# 분류 모델 파인튜닝을 위해 Pretrained BERT에 추가 데이터셋 학습 필요\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "chatbot_data = pd.read_csv('ChatbotData.csv',encoding=\"utf-8\")\n",
    "print(chatbot_data.shape)\n",
    "print(chatbot_data.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ad50bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n",
      "                        Q                      A  label\n",
      "6212                불면증인가  생각이 너무 많아서 그런 것 같습니다.      0\n",
      "648                    텅빔            다시 채워질 거예요.      1\n",
      "2443         마음에 하나도 안 들어              그럴 때가 있죠.      0\n",
      "7580         방금 헤어지고 왔습니다              맘고생 많았어요.      1\n",
      "7962  이별 일주일 후 ! 우리의 이야기.        각자의 삶을 살고 있겠지요.      1\n"
     ]
    }
   ],
   "source": [
    "# 현재 3개의 클래스로 라벨링 되어 있음\n",
    "# 이진분류가 목표이기 때문에 '1'과 '2'로 라벨링 되어 있는 데이터를 모두 '1'로 변경\n",
    "chatbot_data.loc[(chatbot_data['label'] == 2), 'label'] = 1  #라벨 1과 2는 모두 1로 통일\n",
    "\n",
    "# 현재 데이터셋은 0~5290 까지의 행이 모두 클래스가 '0'이고 5291행부터 마지막 행까지 모두 클래스가 '1' 임\n",
    "# 데이터셋을 클래스가 '0'인 데이터와 '1'인 데이터로 랜덤으로 골고루 섞는 코드\n",
    "chatbot_data_shuffled = chatbot_data.sample(frac=1).reset_index(drop=True)\n",
    "print(chatbot_data_shuffled.shape)\n",
    "\n",
    "#train data & test data 로드 \n",
    "train = chatbot_data_shuffled[:9000]\n",
    "test = chatbot_data_shuffled[9000:]\n",
    "print(train.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74856099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "['[CLS] 여친 생겼는데 호칭 어떻게 함? [SEP]', '[CLS] 정시 퇴근하고 집에 가는 중 [SEP]', '[CLS] 짝남은 나한테 관심이 없는 것 같애. [SEP]', '[CLS] 여자친구가 너무 무뚝뚝해 [SEP]', '[CLS] 어렸을때 공부할껄 [SEP]']\n",
      "[1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 현재 데이터셋을 BERT 모델의 입력 데이터 형식으로 변경하기 위한 전처리\n",
    "\n",
    "# BERT 분류 모델의 경우 : [CLS] = 문장의 처음, [SEP] = 문장의 끝\n",
    "# 각 문장의 앞마다 [CLS]를 붙이는 작업 필요, 문장의 종료는 [SEP]를 붙임\n",
    "\n",
    "# CLS, SEP 붙이기 (문장의 시작, 끝)\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in train.Q]\n",
    "print(len(sentences))\n",
    "print(sentences[:5])\n",
    "\n",
    "# '0'과 '1'의 라벨이 들어있는 컬럼을 'labels'이라는 array에 따로 저장\n",
    "labels = train['label'].values\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e39f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지', '##바', '##이', '##크']\n",
      "9000\n",
      "[CLS] 여친 생겼는데 호칭 어떻게 함? [SEP]\n",
      "['[CLS]', '여', '##친', '생', '##겼', '##는데', '호', '##칭', '어', '##떻', '##게', '함', '?', '[SEP]']\n",
      "(9000, 128)\n",
      "[   101   9565  55358   9420 118637  41850   9985  52094   9546 118837\n",
      "  14153   9956    136    102      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# BERT Tokenizer 테스트\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_lower_case=False)\n",
    "result = tokenizer.tokenize('지바이크')\n",
    "print(result)\n",
    "\n",
    "# BERT Tokenizer (WordPiece)를 이용하여 전체 데이터에 토크나이징을 수행\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(s) for s in sentences]\n",
    "print(len(tokenized_texts))\n",
    "\n",
    "print(sentences[0])  #토크나이징 전\n",
    "print(tokenized_texts[0]) #토크나이징 후\n",
    "\n",
    "# 문장의 최대 시퀀스를 설정하고 정수 인코딩과 제로 패딩 수행\n",
    "#MAX_LEN = 512 #BERT에서 지원하는 문장의 최대 길이 = 512\n",
    "MAX_LEN = 128\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(input_ids.shape)\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa3fd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Attention Mask (어텐션 마스크) 필요\n",
    "# 0 값을 가지는 패딩 토큰에 대해서 어텐션 연산을 불필요하게 수행하지 않도록\n",
    "# 단어와 패딩 토큰을 구분할 수 있도록 패딩된 데이터가 있을 때 위치에 따라,\n",
    "# 패딩된 값은 '0', 패딩되지 않은 단어는 '1'을 갖는 시리얼 데이터를 생성\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(len(attention_masks))\n",
    "print(attention_masks[0])\n",
    "\n",
    "# 결과 예) 패딩된 데이터 = [40311, 9435, 102, 0, 0]\n",
    "# >>어텐션 마스크 = [1.0, 1.0, 1.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a32536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 모두 파이토치 텐서로 변환\n",
    "# Train Set 전처리 : 어텐선 마스크를 포함 데이터셋을 훈련셋과 검증셋으로 분리\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=2000, \n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2000, \n",
    "                                                       test_size=0.1)     \n",
    "                                                       \n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe61e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set 전처리\n",
    "# [CLS] + 문장 + [SEP]\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in test.Q]\n",
    "\n",
    "# 라벨 데이터\n",
    "labels = test['label'].values\n",
    "\n",
    "# Word 토크나이저 토큰화\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# 시퀀스 설정 및 정수 인덱스 변환 & 패딩\n",
    "#MAX_LEN = 512 #BERT에서 지원하는 문장의 최대 길이 = 512\n",
    "MAX_LEN = 128\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# 어텐션 마스크\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "    \n",
    "# 파이토치 텐서로 변환\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# 배치 사이즈 설정 및 데이터 설정\n",
    "batch_size = 32\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c6addc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/gbike/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BERT Model Loading\n",
    "'''\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "'''\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "#model.cuda()\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률(learning rate)\n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "\n",
    "# 에폭수\n",
    "epochs = 4\n",
    "\n",
    "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 스케줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e86a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.44\n",
      "  Training epcoh took: 0:23:23\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  Validation took: 0:00:46\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epcoh took: 0:23:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.86\n",
      "  Validation took: 0:00:46\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epcoh took: 0:23:32\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:00:46\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:23:24\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation took: 0:00:45\n",
      "======================\n",
      "= Training complete! =\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "# BERT Model Training\n",
    "# Note : Tesla T4 generally has a higher number of cores and higher memory bandwidth than an Intel CPU\n",
    "# On a machine with a modern CPU, 16GB RAM, and no GPU, fine-tuning a small BERT model (e.g., BERT-base)\n",
    "# on a moderate-sized dataset (e.g., 10,000 examples) could take several days to a week (7 days)!\n",
    "\n",
    "# 주의사항 : GPU 없는 서버(AI Live Server)에서 GPU 코드 실행시 서버 다운 될 수 있음!\n",
    "# GPU 없이 CPU 8개로 학습할 경우 : 1 에폭 학습에 약 24분 걸림 (데이터셋 사이즈 9000 문장)\n",
    "# Tesla T4 GPU 1개의 경우 : 1 에폭 학습에 약 3분 걸림 (데이터셋 사이즈 9000 문장)\n",
    "\n",
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#랜덤시드 고정\n",
    "seed_val = 1234\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 학습\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # 로스 구함\n",
    "        logits = outputs[0] # The raw outputs of a layer (the final layer)\n",
    "\n",
    "        # CPU로 데이터 이동\n",
    "        #logits = logits.detach().cpu().numpy()\n",
    "        #label_ids = b_labels.to('cpu').numpy()\n",
    "        label_ids = b_labels.numpy()\n",
    "        logits = logits.numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"======================\")\n",
    "print(\"= Training complete! =\")\n",
    "print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4023bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.88\n",
      "Test took: 0:02:23\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋 평가\n",
    "# GPU 없이 CPU 8개로 학습할 경우 : 결과 예측에 약 2분 걸림 (데이터셋 사이즈 약 2000 문장)\n",
    "# Tesla T4 GPU 1개의 경우 : 결과 예측에 약 0.2초 걸림 (데이터셋 사이즈 약 2000 문장)\n",
    "\n",
    "#시작 시간 설정\n",
    "t0 = time.time()\n",
    "\n",
    "# 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "# 변수 초기화\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    # 경과 정보 표시\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    # 배치를 GPU에 넣음\n",
    "    #batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # 배치에서 데이터 추출\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    \n",
    "    # 로스 구함\n",
    "    logits = outputs[0] # The raw outputs of a layer (the final layer)\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    #logits = logits.detach().cpu().numpy()\n",
    "    #label_ids = b_labels.to('cpu').numpy()\n",
    "    label_ids = b_labels.numpy()\n",
    "    logits = logits.numpy()\n",
    "    \n",
    "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed97661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 문장 테스트\n",
    "\n",
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    #MAX_LEN = 512 #BERT에서 지원하는 문장의 최대 길이 = 512\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks\n",
    "\n",
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    #b_input_ids = inputs.to(device)\n",
    "    #b_input_mask = masks.to(device)\n",
    "            \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        '''\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "        '''\n",
    "        outputs = model(inputs, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=masks)\n",
    "\n",
    "    # 로스 구함\n",
    "    logits = outputs[0] # The raw outputs of a layer (the final layer)\n",
    "    logits = logits.numpy()\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    #logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eba1baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1652505  1.2381825]]\n",
      "1번 타입 대화\n"
     ]
    }
   ],
   "source": [
    "# 새로운 문장 테스트 (샘플)\n",
    "\n",
    "logits = test_sentences(['지쿠터 타고갈까'])\n",
    "print(logits)\n",
    "\n",
    "if np.argmax(logits) == 1 :\n",
    "    print(\"1번 타입 대화\")\n",
    "elif np.argmax(logits) == 0 :\n",
    "    print(\"0번 타입 대화\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
