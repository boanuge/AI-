{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552af7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 05:29:58.558419: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 05:29:58.558447: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.7.0\n",
      "Transformers version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 사용\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "# Execution Environment for KLUE-BERT\n",
    "# pip install tensorflow==2.7.0\n",
    "# pip install transformers==4.8.0\n",
    "# TensorFlow version: 2.7.0\n",
    "# Transformers version: 4.8.0\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a9cb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels                                          sentences\n",
      "0       0  Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...\n",
      "1       0  테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...\n",
      "2       2  국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...\n",
      "3       1  새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...\n",
      "4       1  2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...\n",
      "BEFORE: Empty DataFrame\n",
      "Columns: [labels, sentences]\n",
      "Index: []\n",
      "AFTER: Empty DataFrame\n",
      "Columns: [labels, sentences]\n",
      "Index: []\n",
      "총 샘플의 수 : 4827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 경제 뉴스 감정 데이터셋 다운로드\n",
    "# 경제 뉴스 기사 내 4,846개 문장 & 감정 라벨, 0: Neutral 59.27%, 1: Positive 28.22%, 2: Negative 12.51%\n",
    "# https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv\n",
    "dataset = pd.read_csv(\"finance_data_preprocessed.csv\") # UTF-8 encoding\n",
    "print(dataset.head())\n",
    "\n",
    "# 중복 데이터 확인 및 중복 데이터 제거\n",
    "print(\"BEFORE:\",dataset[dataset['sentences'].duplicated()]) # 중복 있음\n",
    "dataset.drop_duplicates(subset = ['sentences'], inplace = True) # 중복 제거\n",
    "print(\"AFTER:\",dataset[dataset['sentences'].duplicated()]) # 중복 없음\n",
    "\n",
    "print('총 샘플의 수 :',len(dataset))\n",
    "dataset['labels'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4933497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 입력 데이터 개수: 3861\n",
      "테스트 입력 데이터 개수: 966\n"
     ]
    }
   ],
   "source": [
    "# 입출력 데이터 분리\n",
    "X_data = dataset['sentences'].tolist()\n",
    "y_data = dataset['labels'].tolist()\n",
    "\n",
    "TEST_SIZE = 0.2 # Train:Test = 8:2 분리\n",
    "RANDOM_STATE = 1234\n",
    "X_train_list, X_test_list, y_train, y_test = train_test_split(X_data, y_data, \n",
    "                                                    test_size = TEST_SIZE, \n",
    "                                                    random_state = RANDOM_STATE, \n",
    "                                                    stratify = y_data)\n",
    "print(f\"훈련 입력 데이터 개수: {len(X_train_list)}\")\n",
    "print(f\"테스트 입력 데이터 개수: {len(X_test_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823829ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 05:30:39.188200: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-28 05:30:39.188224: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-28 05:30:39.188262: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-0-134): /proc/driver/nvidia/version does not exist\n",
      "2023-02-28 05:30:39.190523: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"klue/bert-base\")\n",
    "X_train = tokenizer(X_train_list, truncation=True, padding=True)\n",
    "X_test = tokenizer(X_test_list, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_train),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_test),\n",
    "    y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6613cf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '이', '거래', '이후', ',', '단', '##스케', '은행', '##은', '노르', '##딕', '국가', '##의', '노르', '##딕', '은행', '그룹', '노르', '##데', '##아', '##와', '동등', '##한', '선수', '##가', '된다', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[2, 1504, 4083, 3719, 16, 812, 22807, 3924, 2073, 10735, 3004, 3728, 2079, 10735, 3004, 3924, 4063, 10735, 2147, 2227, 2522, 13002, 2470, 3825, 2116, 3622, 18, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].tokens)\n",
    "print(X_train[0].ids)\n",
    "print(X_train[0].type_ids)\n",
    "print(X_train[0].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54acd906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f273d88a940>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f273d88a940>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "121/121 [==============================] - ETA: 0s - loss: 1.4206 - accuracy: 0.4991WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "121/121 [==============================] - 817s 7s/step - loss: 1.4206 - accuracy: 0.4991 - val_loss: 1.0986 - val_accuracy: 0.5932\n",
      "1/1 [==============================] - 52s 52s/step - loss: 1.0986 - accuracy: 0.5932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.098612666130066, 0.5931677222251892]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=3, from_pt=True)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_dataset.shuffle(10000).batch(32), epochs=epochs, batch_size=64,\n",
    "    validation_data = val_dataset.shuffle(10000).batch(64),\n",
    ")\n",
    "\n",
    "'''\n",
    "callback_earlystop = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001,\n",
    "    patience=2\n",
    ")\n",
    "model.fit(\n",
    "    train_dataset.shuffle(10000).batch(32), epochs=epochs, batch_size=64,\n",
    "    validation_data = val_dataset.shuffle(10000).batch(64),\n",
    "    callbacks = [callback_earlystop]\n",
    ")\n",
    "'''\n",
    "\n",
    "model.evaluate(val_dataset.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e424d749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output/klue-bert-base/tokenizer_config.json',\n",
       " 'output/klue-bert-base/special_tokens_map.json',\n",
       " 'output/klue-bert-base/vocab.txt',\n",
       " 'output/klue-bert-base/added_tokens.json',\n",
       " 'output/klue-bert-base/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장 : \"output/klue-bert-base\" 폴더 미리 만들어 놓을것\n",
    "\n",
    "model.save_pretrained('output/klue-bert-base')\n",
    "tokenizer.save_pretrained('output/klue-bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef521162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at output/klue-bert-base were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at output/klue-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드 및 테스트\n",
    "\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained('output/klue-bert-base')\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained('output/klue-bert-base')\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fccd8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.5566717386245728}, {'label': 'LABEL_1', 'score': 0.2830817699432373}, {'label': 'LABEL_2', 'score': 0.16024644672870636}]]\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "predicted_result = text_classifier('지쿠터 타고갈까')\n",
    "print(predicted_dict)\n",
    "\n",
    "# Get the predicted label name\n",
    "label_map = {0: \"Neutral\", 1: \"Positive\", 2: \"Negative\"}\n",
    "\n",
    "predicted_label_scores = []\n",
    "for prediction_item in predicted_result:\n",
    "    for prediction_dict in prediction_item:\n",
    "        predicted_label_scores.append(prediction_dict['score'])\n",
    "    predicted_label_id = np.argmax(predicted_label_scores)\n",
    "    predicted_label_name = label_map[predicted_label_id]\n",
    "\n",
    "print(predicted_label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886261bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
