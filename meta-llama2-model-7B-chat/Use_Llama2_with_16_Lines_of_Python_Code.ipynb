{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700adacc",
   "metadata": {},
   "source": [
    "# text-generation-webui\n",
    "\n",
    "### https://github.com/oobabooga/text-generation-webui/issues/1534#issuecomment-1574427131\n",
    "\n",
    "#### Update apt package manager and change into home directory\n",
    "\n",
    "sudo apt-get update && cd ~ \n",
    "\n",
    "#### Install pre-requisites\n",
    "\n",
    "sudo apt install unzip &&\n",
    "sudo apt install curl &&\n",
    "sudo apt install cmake -y &&\n",
    "sudo apt install python3-pip -y &&\n",
    "pip3 install testresources # dependency for launchpadlib\n",
    "\n",
    "#### Also gcc-11 and g++-11 need to be installed to overcome this llama-cpp-python compilation issue\n",
    "\n",
    "sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test &&\n",
    "sudo apt install -y gcc-11 g++-11 &&\n",
    "sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 60 --slave /usr/bin/g++ g++ /usr/bin/g++-11 &&\n",
    "pip3 install --upgrade pip &&\n",
    "pip3 install --upgrade setuptools wheel &&\n",
    "sudo apt-get install build-essential &&\n",
    "gcc-11 --version # check if gcc works\n",
    "\n",
    "#### Download the WebUI installer from repository and unpack it\n",
    "\n",
    "wget https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_linux.zip &&\n",
    "unzip oobabooga_linux.zip && \n",
    "rm oobabooga_linux.zip\n",
    "\n",
    "#### change into the downloaded folder and run the installer, this will download the necessary files etc. into a single folder\n",
    "\n",
    "cd oobabooga_linux &&\n",
    "bash start_linux.sh\n",
    "\n",
    "### Running on local URL:  http://127.0.0.1:7860"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d5815",
   "metadata": {},
   "source": [
    "# https://dev.to/0xkoji/using-llama2-with-16-lines-of-python-code-an9\n",
    "## import python package (https://github.com/abetlen/llama-cpp-python)\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ pip3 install --upgrade pip\n",
    "\n",
    "To build llama-cpp-python.git, you can follow the instructions provided in the GitHub repository. Here is a summary of the steps:\n",
    "\n",
    "Clone the repository using the following command:\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ git clone --recurse-submodules git@github.com:abetlen/llama-cpp-python.git\n",
    "\n",
    "Navigate to the cloned directory:\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ cd llama-cpp-python\n",
    "\n",
    "Set the environment variable LLAMA_METAL to 1 if you want to build with Metal support:\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ export LLAMA_METAL=1\n",
    "\n",
    "Set the environment variable CMAKE_ARGS to -DLLAMA_METAL=on if you want to build with Metal support:\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ export CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n",
    "\n",
    "Set the environment variable FORCE_CMAKE to 1:\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ export FORCE_CMAKE=1\n",
    "\n",
    "Install the package using pip:\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ pip install .\n",
    "\n",
    "#### Note: for windows os system\n",
    "\n",
    "set \"CMAKE_ARGS=-DLLAMA_OPENBLAS=on\"\n",
    "\n",
    "set \"FORCE_CMAKE=1\"\n",
    "\n",
    "pip install llama-cpp-python --no-cache-dir\n",
    "\n",
    "#### Note: if you want to try to run a model without GPU, just \"pip install llama-cpp-python\"\n",
    "\n",
    "#### Note: you may try the below command to use a specific working version\n",
    "\n",
    "(base) user@ip-192-168-10-1:~/llama2$ CMAKE_ARGS=\"-DLLAMA_OPENBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16f32ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-08 08:13:44--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q5_K_M.bin\n",
      "Resolving huggingface.co (huggingface.co)... 13.225.128.86, 13.225.128.5, 13.225.128.72, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.225.128.86|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/174e70bcdfbbd872bb3dfce76ad30f3535e08895caee9cfc1b24c4503320c181?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_K_M.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_K_M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1699658025&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTY1ODAyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3LzE3NGU3MGJjZGZiYmQ4NzJiYjNkZmNlNzZhZDMwZjM1MzVlMDg4OTVjYWVlOWNmYzFiMjRjNDUwMzMyMGMxODE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=R6s0bqayi0VG-96JO3jhxIYjkJwqzVL1KIsaU2We-0%7ED5GPEJTzw3%7Edg3CTkOM8zOa5B2zR0LFUAXVUa5qGzSJ3mesn-9GEygN84LZxtgEg-KiKrfafQYEo6r7QDTqjhD18EuuP4v0tY4Va1h3EMjD%7EUFQ%7ENh-6v7WWyj8eqHL5EW4alyL84IrykhOfE8kP2BCMFqgS1bRzilJb2c9Jp69JeX5kKC8u4qu9Yk9wBfopb5A2y4nOz8ZwU0t3aPpo9Ij4CMlbvaaCC3gzOjVLAXRPiINbrZOTrAB9OKXNUSbhrg3KyNj19GXqRsRsS64ZTkl4xfa5V93%7EFBdQOulF8GQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-11-08 08:13:45--  https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/174e70bcdfbbd872bb3dfce76ad30f3535e08895caee9cfc1b24c4503320c181?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q5_K_M.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q5_K_M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1699658025&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTY1ODAyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3LzE3NGU3MGJjZGZiYmQ4NzJiYjNkZmNlNzZhZDMwZjM1MzVlMDg4OTVjYWVlOWNmYzFiMjRjNDUwMzMyMGMxODE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=R6s0bqayi0VG-96JO3jhxIYjkJwqzVL1KIsaU2We-0%7ED5GPEJTzw3%7Edg3CTkOM8zOa5B2zR0LFUAXVUa5qGzSJ3mesn-9GEygN84LZxtgEg-KiKrfafQYEo6r7QDTqjhD18EuuP4v0tY4Va1h3EMjD%7EUFQ%7ENh-6v7WWyj8eqHL5EW4alyL84IrykhOfE8kP2BCMFqgS1bRzilJb2c9Jp69JeX5kKC8u4qu9Yk9wBfopb5A2y4nOz8ZwU0t3aPpo9Ij4CMlbvaaCC3gzOjVLAXRPiINbrZOTrAB9OKXNUSbhrg3KyNj19GXqRsRsS64ZTkl4xfa5V93%7EFBdQOulF8GQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 54.230.61.44, 54.230.61.102, 54.230.61.84, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|54.230.61.44|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9229634688 (8.6G) [application/octet-stream]\n",
      "Saving to: ‘llama-2-13b-chat.ggmlv3.q5_K_M.bin’\n",
      "\n",
      "llama-2-13b-chat.gg 100%[===================>]   8.60G   129MB/s    in 54s     \n",
      "\n",
      "2023-11-08 08:14:39 (164 MB/s) - ‘llama-2-13b-chat.ggmlv3.q5_K_M.bin’ saved [9229634688/9229634688]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you want to try a 7B model\n",
    "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q5_K_M.bin\n",
    "\n",
    "!wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q5_K_M.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61c208",
   "metadata": {},
   "source": [
    "# load model\n",
    "\n",
    "If you use only cpu, you won't need n_gpu_layers=32 option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import ctypes\n",
    "llm =Llama(model_path=\"/home/gbike/AI_test/llama2/llama-2-13b-chat.ggmlv3.q5_K_M.bin\", n_gpu_layers=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34839e10",
   "metadata": {},
   "source": [
    "# set prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3640be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generate lyrics for a romantic love song\"\n",
    "input_prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a charismatics, talented, respectful and honest musician. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e2fe2",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b866830",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(input_prompt,max_tokens=1024)\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb0a86",
   "metadata": {},
   "source": [
    "### output\n",
    " Of course! I'd be happy to help generate lyrics for a love song. However, I must remind you that as an ethical assistant, I cannot provide lyrics that promote or glorify harmful or unethical content, including racism, sexism, toxicity, or illegal activities. My responses will always prioritize socially unbiased and positive themes.\n",
    "With that said, here are some general tips for writing a love song:\n",
    "1. Start with a clear idea of the theme or emotion you want to convey. Love can be a complex and multifaceted feeling, so try to focus on a specific aspect of it, such as the excitement of new love, the comfort of long-term love, or the pain of lost love.\n",
    "2. Use sensory language to create vivid imagery and evoke emotions. Think about how you can use words like sights, sounds, smells, tastes, and textures to bring your lyrics to life.\n",
    "3. Play with rhyme and meter to create a catchy and memorable melody. Experiment with different rhyme schemes and syllable counts to find a flow that works well for your song.\n",
    "4. Be honest and authentic in your lyrics. Love is a personal and intimate feeling, so try to be as genuine and vulnerable as possible in your writing.\n",
    "\n",
    "With these tips in mind, here is a love song I came up with:\n",
    "Verse 1:\n",
    "Your eyes are like the sun, shining bright\n",
    "Warming my heart with their light\n",
    "Your touch ignites a fire within me\n",
    "Melting my fears and setting me free"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
