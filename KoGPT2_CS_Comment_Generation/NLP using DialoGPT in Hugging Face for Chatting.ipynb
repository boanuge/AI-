{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad651ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0 #tensorflow 2.10.0 <-- 2.12.0 (버전 다운그레이드 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 07:11:50.951797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-31 07:11:50.951827: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/gbike/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[모델 로딩 시간]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 07:11:52.989707: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-31 07:11:52.989733: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-31 07:11:52.989753: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-31 07:11:52.989955: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 07:11:53.002289: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/kogpt2-마침표있음-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time duration(in seconds): 1.968888933013659\n",
      ">> User:Jesus\n",
      "[문장 생성 시간]\n",
      "Time duration(in seconds): 7.126206719025504\n",
      ">> GPT: . All Rights Reserved.\n",
      " @ CDT(2023-03-31T07:12:04.904661)\n",
      ">> User:BYE\n"
     ]
    }
   ],
   "source": [
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from timeit import default_timer\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "print(\"[모델 로딩 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "'''\n",
    "# We assume the model and the tokenizer has been saved by the below code.\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "'''\n",
    "loaded_model = TFGPT2LMHeadModel.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "def ouputGPT(prompt=\"\"):\n",
    "    print(\"[문장 생성 시간]\")\n",
    "    start = default_timer()\n",
    "\n",
    "    '''\n",
    "    Temperature: 답변의 창의성과 무작위성을 조정하는 값입니다. 낮을 수록 (예: 0.05) 보다 사실에 근거한 정확한 답변을 제공하고, 높을 수록 (예: 1) 보다 창의적인 결과물을 생성해줍니다. \n",
    "    Top_p: 답변의 무작위성을 제어하는 조정 값입니다. Temperature와 마찬가지로 값이 낮을 수록 답변이 보다 정확하고 높아질 수록 창의적이고 광범위해집니다. do_sample=True를 통해 이 중 랜덤한 샘플값을 선택합니다.\n",
    "    frequency_penalty: 값이 높을 수록 AI가 흔하지 않은 단어를 답변에 포함할 가능성을 제어합니다. 반대로 값이 0이라면 페널티가 전혀 주어지지 않아서 AI가 흔히 사용되지 않는 단어를 답변에 포함할 가능성이 높아지게 됩니다. \n",
    "    presence_penalty: 값이 높을 수록 AI가 유사하거나 동일한 단어 및 문구를 답변 시 반복할 가능성을 제어합니다. 이 또한 반대로 값이 0이라면 페널티가 전혀 주어지지 않아서 AI가 단어나 문구를 반복할 가능성이 높아지게 됩니다. \n",
    "    '''\n",
    "    #prompt = \"예수님\"\n",
    "    input_ids = loaded_tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                       max_length=30+input_ids.shape[1],\n",
    "                                       early_stopping=True)\n",
    "    generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    split_generated_text = generated_text.split(prompt)\n",
    "    if len(split_generated_text) > 1:\n",
    "        generated_text = split_generated_text[1]\n",
    "    # Trim the sentences after the last period(.)\n",
    "    text_to_remove = generated_text.split('.')[-1]\n",
    "    generated_text = generated_text.replace(text_to_remove,'')\n",
    "\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    return generated_text\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "    print(\">> GPT: {}\".format( ouputGPT(prompt) ))\n",
    "    print_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving microsoft/DialoGPT-large @ CDT(2023-04-03T09:54:32.972222)\n",
      "Loading DialoGPT-large-by-Microsoft-Pytorch @ CDT(2023-04-03T09:55:07.304095)\n",
      ">> User:Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 09:55:23.551781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-03 09:55:23.551810: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/gbike/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hiya\n",
      ">> User:Do you speak Korean?\n",
      "DialoGPT: I do\n",
      ">> User:굉장해요.\n",
      "DialoGPT: I'm not sure if you're joking or not.\n",
      ">> User:I see.\n",
      "DialoGPT: I see what you did there.\n",
      ">> User:What do you mean?\n",
      "DialoGPT: I see what you did there.\n",
      " @ CDT(2023-04-03T09:56:08.230393)\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Saving microsoft/DialoGPT-large\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer.save_pretrained(\"DialoGPT-large-by-Microsoft-Pytorch\")\n",
    "model.save_pretrained(\"DialoGPT-large-by-Microsoft-Pytorch\")\n",
    "\n",
    "print_current_datetime(\"Loading DialoGPT-large-by-Microsoft-Pytorch\")\n",
    "\n",
    "tokenizer_loaded = tokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Pytorch\")\n",
    "model_loaded = model.from_pretrained(\"DialoGPT-large-by-Microsoft-Pytorch\")\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer_loaded.encode(input(\">> User:\") + tokenizer_loaded.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens,\n",
    "    chat_history_ids = model_loaded.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer_loaded.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "print_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-03T12:08:09.637445)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi and thank you for the help!\n",
      "Response 1: Hi and thank you for the help!\n",
      "Response 2: Hi and welcome to the world of real life.\n",
      "Response 3: Hi\n",
      "Response 4: Hi\n",
      "Response 5: Hi\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "'''\n",
    "print_current_datetime(\"Loading pre-trained microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "'''\n",
    "print_current_datetime(\"Loading saved DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Hi\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(\n",
    "    input_ids,\n",
    "    max_length=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "    pad_token_id=loaded_tokenizer.eos_token_id\n",
    ")\n",
    "print(loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "# The below code loops through the generated responses and print them out with a response number.\n",
    "for i, chat_history in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(chat_history, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Hi\n",
      "[문장 생성 시간]\n",
      "Time duration(in seconds): 3.3551467680372298\n",
      ">> GPT: Hi this me :D\n",
      " @ CDT(2023-04-03T12:25:59.818459)\n",
      ">> User:Do you speak Korean?\n",
      "[문장 생성 시간]\n",
      "Time duration(in seconds): 0.7616070339572616\n",
      ">> GPT: Do you speak Korean?\n",
      " @ CDT(2023-04-03T12:26:12.380774)\n",
      ">> User:굉장해요.\n",
      "[문장 생성 시간]\n",
      "Time duration(in seconds): 5.190187129017431\n",
      ">> GPT: 굉장해요.com.in.nl!!\n",
      " @ CDT(2023-04-03T12:26:22.268054)\n",
      ">> User:I see.\n",
      "[문장 생성 시간]\n",
      "Time duration(in seconds): 4.091038952989038\n",
      ">> GPT: I see.You're a real boy\n",
      " @ CDT(2023-04-03T12:26:40.649518)\n",
      ">> User:bye\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "def ouputGPT(prompt=\"\"):\n",
    "    print(\"[문장 생성 시간]\")\n",
    "    start = default_timer()\n",
    "\n",
    "    input_ids = loaded_tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                       max_length=1024,\n",
    "                                       temperature=0.9,\n",
    "                                       top_p=0.9,\n",
    "                                       do_sample=True,\n",
    "                                       num_return_sequences=1,\n",
    "                                       pad_token_id=loaded_tokenizer.eos_token_id\n",
    "                                      )\n",
    "    generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    return generated_text\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "    print(\">> GPT: {}\".format( ouputGPT(prompt) ))\n",
    "    print_current_datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
