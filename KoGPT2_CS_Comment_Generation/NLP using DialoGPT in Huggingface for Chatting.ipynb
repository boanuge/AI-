{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0 #tensorflow 2.10.0 <-- 2.12.0 (버전 다운그레이드 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c02a53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-25T14:22:00.786168)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Microsoft DialoGPT Model @ CDT(2023-04-25T14:22:12.347576)\n",
      "Loading Microsoft DialoGPT Model @ CDT(2023-04-25T14:22:38.448764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-25T14:22:46.148085)\n",
      ">> User:Hi\n",
      "Response 1: Hi\n",
      "Response 2: Hi, I'm not sure if it's possible to change the name of the name of the name.\n",
      "Response 3: Hi\n",
      "Response 4: Hi, I think that's the first time this season, I've seen it, and it's a good thing.I'm glad you're having a good time\n",
      "Response 5: Hi. I would like to know how to get a copy of the game. it's not really a game\n",
      "Time duration(in seconds): 29.757516394005506\n",
      ">> GPT: \n",
      " @ CDT(2023-04-25T14:23:18.788136)\n",
      ">> User:Do you speak Korean?\n",
      "Response 1: Do you speak Korean?\n",
      "Response 2: Do you speak Korean?\n",
      "Response 3: Do you speak Korean?\n",
      "Response 4: Do you speak Korean?\n",
      "Response 5: Do you speak Korean?\n",
      "Time duration(in seconds): 1.0472906050126767\n",
      ">> GPT: \n",
      " @ CDT(2023-04-25T14:23:32.307827)\n",
      ">> User:굉장해요.\n",
      "Response 1: 굉장해요.You've been waiting to use that one for a while now.\n",
      "Response 2: 굉장해요.jpg.\n",
      "Response 3: 굉장해요.gif\n",
      "Response 4: 굉장해요.That's how you spell it.\n",
      "Response 5: 굉장해요.He was very good at being a politician.\n",
      "Time duration(in seconds): 12.9528840569983\n",
      ">> GPT: You've been waiting to use that one for a while now.\n",
      " @ CDT(2023-04-25T14:23:57.514517)\n",
      ">> User:I see.\n",
      "Response 1: I see.You're the only one who noticed.\n",
      "Response 2: I see.I'm not the only one who doesn't have an Android phone.\n",
      "Response 3: I see.The game is set up for a 3rd person shooter.\n",
      "Response 4: I see.He got hit by a car.\n",
      "Response 5: I see.This is a really good idea\n",
      "Time duration(in seconds): 13.878951505990699\n",
      ">> GPT: You're the only one who noticed.\n",
      " @ CDT(2023-04-25T14:24:18.993482)\n",
      ">> User:What do you mean?\n",
      "Response 1: What do you mean?\n",
      "Response 2: What do you mean?\n",
      "Response 3: What do you mean?\n",
      "Response 4: What do you mean?\n",
      "Response 5: What do you mean?\n",
      "Time duration(in seconds): 1.0329199529951438\n",
      ">> GPT: \n",
      " @ CDT(2023-04-25T14:24:55.186302)\n",
      ">> User:bye\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "from timeit import default_timer\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "print_current_datetime(\"Saving Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "print_current_datetime(\"Loading Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer_loaded = tokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model_loaded = model.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "def generate_ouput(prompt=\"\"):\n",
    "\n",
    "    start = default_timer()\n",
    "\n",
    "    input_ids = tokenizer_loaded.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = model_loaded.generate(input_ids=input_ids,\n",
    "                                       max_length=1024,\n",
    "                                       temperature=0.7,\n",
    "                                       top_p=0.9,\n",
    "                                       do_sample=True,\n",
    "                                       num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "                                       pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "    generated_text = tokenizer_loaded.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    end = default_timer()\n",
    "\n",
    "    # num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "    # The below code loops through the generated responses and print them out with a response number.\n",
    "    for i, return_sequence in enumerate(output_ids):\n",
    "        print(f'Response {i+1}: {tokenizer_loaded.decode(return_sequence, skip_special_tokens=True)}')\n",
    "\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    return generated_text\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "\n",
    "    generated_text = generate_ouput(prompt)\n",
    "    split_generated_text = generated_text.split(prompt)\n",
    "    if len(split_generated_text) > 1:\n",
    "        generated_text = split_generated_text[1]\n",
    "    # Trim the sentences after the last period(.)\n",
    "    text_to_remove = generated_text.split('.')[-1]\n",
    "    generated_text = generated_text.replace(text_to_remove,'')\n",
    "\n",
    "    print(\">> GPT: {}\".format( generated_text ))\n",
    "    print_current_datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
