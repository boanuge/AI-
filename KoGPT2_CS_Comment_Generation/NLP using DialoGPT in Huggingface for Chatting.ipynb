{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0 #tensorflow 2.10.0 <-- 2.12.0 (버전 다운그레이드 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c02a53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-25T14:22:00.786168)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Microsoft DialoGPT Model @ CDT(2023-04-25T14:22:12.347576)\n",
      "Loading Microsoft DialoGPT Model @ CDT(2023-04-25T14:22:38.448764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-25T14:22:46.148085)\n",
      ">> User:Hi\n",
      "Response 1: Hi\n",
      "Response 2: Hi, I'm not sure if it's possible to change the name of the name of the name.\n",
      "Response 3: Hi\n",
      "Response 4: Hi, I think that's the first time this season, I've seen it, and it's a good thing.I'm glad you're having a good time\n",
      "Response 5: Hi. I would like to know how to get a copy of the game. it's not really a game\n",
      "Time duration(in seconds): 29.757516394005506\n",
      ">> GPT: \n",
      " @ CDT(2023-04-25T14:23:18.788136)\n",
      ">> User:Do you speak Korean?\n",
      "Response 1: Do you speak Korean?\n",
      "Response 2: Do you speak Korean?\n",
      "Response 3: Do you speak Korean?\n",
      "Response 4: Do you speak Korean?\n",
      "Response 5: Do you speak Korean?\n",
      "Time duration(in seconds): 1.0472906050126767\n",
      ">> GPT: \n",
      " @ CDT(2023-04-25T14:23:32.307827)\n",
      ">> User:굉장해요.\n",
      "Response 1: 굉장해요.You've been waiting to use that one for a while now.\n",
      "Response 2: 굉장해요.jpg.\n",
      "Response 3: 굉장해요.gif\n",
      "Response 4: 굉장해요.That's how you spell it.\n",
      "Response 5: 굉장해요.He was very good at being a politician.\n",
      "Time duration(in seconds): 12.9528840569983\n",
      ">> GPT: You've been waiting to use that one for a while now.\n",
      " @ CDT(2023-04-25T14:23:57.514517)\n",
      ">> User:I see.\n",
      "Response 1: I see.You're the only one who noticed.\n",
      "Response 2: I see.I'm not the only one who doesn't have an Android phone.\n",
      "Response 3: I see.The game is set up for a 3rd person shooter.\n",
      "Response 4: I see.He got hit by a car.\n",
      "Response 5: I see.This is a really good idea\n",
      "Time duration(in seconds): 13.878951505990699\n",
      ">> GPT: You're the only one who noticed.\n",
      " @ CDT(2023-04-25T14:24:18.993482)\n",
      ">> User:What do you mean?\n",
      "Response 1: What do you mean?\n",
      "Response 2: What do you mean?\n",
      "Response 3: What do you mean?\n",
      "Response 4: What do you mean?\n",
      "Response 5: What do you mean?\n",
      "Time duration(in seconds): 1.0329199529951438\n",
      ">> GPT: \n",
      " @ CDT(2023-04-25T14:24:55.186302)\n",
      ">> User:bye\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "from timeit import default_timer\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "print_current_datetime(\"Saving Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "print_current_datetime(\"Loading Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer_loaded = tokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model_loaded = model.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "def generate_ouput(prompt=\"\"):\n",
    "\n",
    "    start = default_timer()\n",
    "\n",
    "    input_ids = tokenizer_loaded.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = model_loaded.generate(input_ids=input_ids,\n",
    "                                       max_length=1024,\n",
    "                                       temperature=0.7,\n",
    "                                       top_p=0.9,\n",
    "                                       do_sample=True,\n",
    "                                       num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "                                       pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "    generated_text = tokenizer_loaded.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    end = default_timer()\n",
    "\n",
    "    # num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "    # The below code loops through the generated responses and print them out with a response number.\n",
    "    for i, return_sequence in enumerate(output_ids):\n",
    "        print(f'Response {i+1}: {tokenizer_loaded.decode(return_sequence, skip_special_tokens=True)}')\n",
    "\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    return generated_text\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "\n",
    "    generated_text = generate_ouput(prompt)\n",
    "    split_generated_text = generated_text.split(prompt)\n",
    "    if len(split_generated_text) > 1:\n",
    "        generated_text = split_generated_text[1]\n",
    "    # Trim the sentences after the last period(.)\n",
    "    text_to_remove = generated_text.split('.')[-1]\n",
    "    generated_text = generated_text.replace(text_to_remove,'')\n",
    "\n",
    "    print(\">> GPT: {}\".format( generated_text ))\n",
    "    print_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aaf1c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 15:26:03.226837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 15:26:03.353421: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-25 15:26:03.353441: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-25 15:26:03.383047: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-25 15:26:03.934319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 15:26:03.934382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-25 15:26:03.934389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-25 15:26:05.591321: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-25 15:26:05.591353: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-25 15:26:05.591372: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-04-25 15:26:05.591579: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 51s 51s/step - loss: 11.5285\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 9.0290\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 7.7521\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 6.5265\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.7008\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 5.1092\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.4682\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.7823\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.1973\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.6637\n",
      "Response 1: What is a great language? Java, Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java Java\n",
      "Response 2: What is a great language?Bravo, you're a great person.IAmA.jpg\n",
      "Response 3: What is a great language?\n",
      "Response 4: What is a great language? languages that are great for the English language. Mandarin is a great language to learn. Mandarin Mandarin is a great language to learn Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin'IsL.IGI Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin'IuII Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin'IIsWh Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin Mandarin''HThanksO'IyouIuThankIIsIThanks'YouIuBIsIThanks'uthe.BILyoutheIMuyouImIt'youIYouyouOIIuIsyouItyouHuuhyou'IIOIsYouIyouThankyouII.YouI'ILTheYouIsYouIIWhatWhIyou'IGGItIyouThanksIyouIIIyouII'It'IwhatItYouItIyouyouIIIOIsIIIIII'IsItisThanksIuI'IsuIOTheIItIyouI.YouIsyouIThanksIIIiI'IyouYouyouYouYouIYouCYouIuItIsIO'IyouHI\n",
      "Response 5: What is a great language?Banned from r all because I'm a brit. English isn't just for you.English is a great language and it's not even a language, it's a language.English is a great language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language you. language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language language Language language language language language language language language language language language language language language languages language language language language language language language language language language language language language language language language. language language language language language language language language language. language language language language language language language languageLanguage language language language Languages languages language language language language language language language language language language language language language language translated.a....a., languages language language language language language language language Languages language language language language language language language language language language language language Language is..r...Ayou Language language language language language you language language language language Language language language language language language language language language Language language.Language language language language language languages language language language language language language language language language...Language language language language language language language Language language language language language language language language Language language language language language language...Language language language language language language language language language language languages language language language Language language language language language language language.r.I language language language language language english language language language language language language language\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "# Fine-tune the model with the below sentences\n",
    "sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I'm doing well, thanks for asking.\",\n",
    "    \"What have you been up to lately?\",\n",
    "    \"Not much, just working on some projects.\",\n",
    "    \"That sounds interesting. What kind of projects?\",\n",
    "    \"Just some coding projects for work.\",\n",
    "    \"Ah, I see. What kind of coding do you do?\",\n",
    "    \"Mostly Python and Java.\",\n",
    "    \"Cool, I've been meaning to learn Python.\",\n",
    "    \"It's a great language. You should definitely give it a try!\"\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.encode(\"\\n\".join(sentences), return_tensors='tf')\n",
    "outputs = model(input_ids)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=[loss, loss])\n",
    "\n",
    "model.fit(input_ids, input_ids, epochs=10)\n",
    "\n",
    "# Generate fine-tuned outputs\n",
    "input_ids = tokenizer.encode(\"What is a great language?\", return_tensors='tf')\n",
    "\n",
    "finetuned_output = model.generate(input_ids=input_ids,\n",
    "                   max_length=1024,\n",
    "                   temperature=0.7,\n",
    "                   top_p=0.9,\n",
    "                   do_sample=True,\n",
    "                   num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "                   pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "for i, return_sequence in enumerate(finetuned_output):\n",
    "    print(f'Response {i+1}: {tokenizer.decode(return_sequence, skip_special_tokens=True)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
