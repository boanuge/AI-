{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0 #tensorflow 2.10.0 <-- 2.12.0 (버전 다운그레이드 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8403eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0fea061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-24T15:23:15.012303)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42862f6b3cab455bbf50e064cd9b89ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/199 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949b783070894e2eabf62e4c1536628b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/13.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23980b0bf624dd99a21c0669418db01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a61e6946564e4788fcc814ca1ebc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4725f1e64d4cbd934ccb09cfc2607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:25:01.715799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 15:25:03.252919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-24 15:25:03.252937: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-24 15:25:03.403700: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-24 15:25:06.046152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 15:25:06.046249: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 15:25:06.046258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you speak Korean? no\n",
      " @ CDT(2023-04-24T15:25:09.532013)\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bigscience/bloomz-3b\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "\n",
    "prompt = \"Do you speak Korean?\"\n",
    "result_length = 50\n",
    "\n",
    "tokenin = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokenout = model.generate(tokenin[\"input_ids\"], max_length=result_length)\n",
    "output = tokenizer.decode(tokenout[0], skip_special_tokens=True)\n",
    "print(output)\n",
    "\n",
    "print_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c0b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-24T15:33:25.249526)\n",
      "Saving BigScience AI team : bigscience/bloomz-3b model @ CDT(2023-04-24T15:34:44.800720)\n",
      "Loading BigScience AI team : bigscience/bloomz-3b model @ CDT(2023-04-24T15:36:42.125495)\n"
     ]
    }
   ],
   "source": [
    "# BigScience/BloomZ-3b(file size = 6GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-3b\")\n",
    "\n",
    "print_current_datetime(\"Saving BigScience AI team : bigscience/bloomz-3b model\")\n",
    "\n",
    "tokenizer.save_pretrained(\"BloomZ-3b-by-BigScience\")\n",
    "model.save_pretrained(\"BloomZ-3b-by-BigScience\")\n",
    "\n",
    "print_current_datetime(\"Loading BigScience AI team : bigscience/bloomz-3b model\")\n",
    "\n",
    "# 메모리 부족으로 아래 코드들 실행되지 못함 (현재 메모리 : 32G)\n",
    "tokenizer_loaded = tokenizer.from_pretrained(\"BloomZ-3b-by-BigScience\")\n",
    "model_loaded = model.from_pretrained(\"BloomZ-3b-by-BigScience\")\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer_loaded.encode(input(\">> User:\") + tokenizer_loaded.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens,\n",
    "    chat_history_ids = model_loaded.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"BloomZ: {}\".format(tokenizer_loaded.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "print_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d32d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BigScience AI team : bigscience/bloomz-3b model @ CDT(2023-04-24T15:46:21.487897)\n",
      " @ CDT(2023-04-24T15:48:40.410316)\n",
      ">> User:Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:52:24.405683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 15:52:25.535691: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-24 15:52:25.535715: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-24 15:52:25.653098: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-24 15:52:27.493615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 15:52:27.493687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 15:52:27.493696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloomZ: A. The first step is to find the number of rows and columns of the image\n",
      ">> User:Do you speak Korean?\n",
      "BloomZ: The first step is to find the number of rows and columns of the image. This is done by using the function imread().\n",
      ">> User:굉장해요.\n",
      "BloomZ: The first step is to find the number of rows and columns of the image. This is done by using the function imread().\n",
      ">> User:I see.\n",
      "BloomZ: The first thing I did was to try to find the number of rows and columns of the image. This is done by using the function imread().\n",
      ">> User:What do you mean?\n",
      "BloomZ: The first thing I did was to try to find the number of rows and columns of the image. This is done by using the function imread().\n",
      " @ CDT(2023-04-24T15:53:34.683834)\n"
     ]
    }
   ],
   "source": [
    "# BigScience/BloomZ-3b(file size = 6GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print_current_datetime(\"Loading BigScience AI team : bigscience/bloomz-3b model\")\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(\"BloomZ-3b-by-BigScience\")\n",
    "model_loaded = AutoModelForCausalLM.from_pretrained(\"BloomZ-3b-by-BigScience\")\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer_loaded.encode(input(\">> User:\") + tokenizer_loaded.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens,\n",
    "    chat_history_ids = model_loaded.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"BloomZ: {}\".format(tokenizer_loaded.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "print_current_datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
