{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46847233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:30:58.056841: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-15 16:30:58.056870: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/gbike/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.7.0\n",
      "Transformers version: 4.21.0\n"
     ]
    }
   ],
   "source": [
    "# Execution Environment for KLUE-BERT\n",
    "# pip install tensorflow==2.7.0\n",
    "# pip install transformers==4.21.0\n",
    "# TensorFlow version: 2.7.0\n",
    "# Transformers version: 4.21.0\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime():\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    print(\"CDT(Current Date and Time):\", datetime_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8381beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 후 데이터셋 : 총 117,609개\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Read Excel data\n",
    "df = pd.read_excel(\"[데이터셋] CS 신고관리 코멘트생성용 @ 전처리.xlsx\", sheet_name=\"전처리\")\n",
    "\n",
    "# Write CSV data\n",
    "df.to_csv(\"[데이터셋] CS 신고관리 코멘트생성용 @ 전처리.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "# 파일에서 \"\\t\" 을 \"|\" 변환 후 Prompt |[SEP]| Target Text [다음줄] 포맷으로 변경\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e01dd80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:31:01.948964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-15 16:31:01.948990: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-15 16:31:01.949008: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-15 16:31:01.949221: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 16:31:01.962120: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.9.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'lm_head.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.4.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 총 샘플 개수: 117609\n",
      "데이터셋 프롬프트 최대 토큰 길이: 137\n",
      "데이터셋 타겟문장 최대 토큰 길이: 278\n"
     ]
    }
   ],
   "source": [
    "# GPTv2 모델 사용\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load pre-trained GPT2 model and tokenizer\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\", from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# Load the dataset of prompts and target texts\n",
    "# Prompt : a sentence, phrase, or a set of keywords that\n",
    "# act as a starting point for generating a response text\n",
    "with open(\"dataset_for_cs_comment_generation.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [line.strip().split(\"|[SEP]|\") for line in f.readlines()]\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.seed(1234)\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print(\"데이터셋 총 샘플 개수:\", len(dataset))\n",
    "print(dataset[:2][:])\n",
    "\n",
    "sample_prompt = []\n",
    "sample_target = []\n",
    "for prompt, target in dataset:\n",
    "    sample_prompt.append(prompt)\n",
    "    sample_target.append(target)\n",
    "print(sample_prompt[:2])\n",
    "print(sample_target[:2])\n",
    "\n",
    "input_ids = []\n",
    "output_ids = []\n",
    "for prompt, target in dataset:\n",
    "    # Encode the prompt and target as input and output sequences\n",
    "    # Not using special tokens such as [CLS], [SEP], etc.\n",
    "    prompt_encoded = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    target_encoded = tokenizer.encode(target, add_special_tokens=False)\n",
    "    # Add the encoded sequences to the input and output lists\n",
    "    input_ids.append(prompt_encoded)\n",
    "    output_ids.append(target_encoded)\n",
    "\n",
    "max_seq_length = 0\n",
    "for arr in input_ids:\n",
    "    if len(arr) > max_seq_length:\n",
    "        max_seq_length = len(arr)\n",
    "print(\"데이터셋 프롬프트 최대 토큰 길이:\", max_seq_length)\n",
    "max_seq_length = 0\n",
    "for arr in output_ids:\n",
    "    if len(arr) > max_seq_length:\n",
    "        max_seq_length = len(arr)\n",
    "print(\"데이터셋 타겟문장 최대 토큰 길이:\", max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b962e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터셋 개수: 117609\n",
      "훈련 데이터셋 수: 105848\n",
      "검증 데이터셋 수: 11761\n",
      "Time duration(in seconds): 31.875500281108543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwith np.printoptions(threshold=np.inf):\\n    print(train_input_ids.shape)\\n    print(train_input_ids[:2])\\n    print(train_output_ids.shape)\\n    print(train_output_ids[:2])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataset to tokenized sequences\n",
    "input_ids = []\n",
    "output_ids = []\n",
    "\n",
    "start = default_timer()\n",
    "\n",
    "for prompt, target in dataset:\n",
    "    # Encode the prompt and target as input and output sequences\n",
    "    # Not using special tokens such as [CLS], [SEP], etc.\n",
    "    prompt_encoded = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    target_encoded = tokenizer.encode(target, add_special_tokens=False)\n",
    "    input_seq = prompt_encoded\n",
    "    output_seq = target_encoded\n",
    "    # Truncate sequences that are too long\n",
    "    # For GPT2, the default value for max_seq_length is 1024\n",
    "    # Using the default value (1024) causes OOM error\n",
    "    # max_seq_length = model.config.n_positions\n",
    "    max_seq_length = 280 # 데이터셋 타겟문장 최대 토큰 길이 (참고)\n",
    "    if len(input_seq) > max_seq_length:\n",
    "        input_seq = input_seq[:max_seq_length]\n",
    "    if len(output_seq) > max_seq_length:\n",
    "        output_seq = output_seq[:max_seq_length]\n",
    "    # Pad sequences that are too short\n",
    "    # The default value for padding in GPT2 tokenizer is 0\n",
    "    # Using tokenizer.pad_token_id value (None) causes error\n",
    "    # because None str is not int64 type, which is required\n",
    "    padding_length_input_seq = max_seq_length - len(input_seq)\n",
    "    if ( padding_length_input_seq > 0 ):\n",
    "        input_seq += [0] * padding_length_input_seq\n",
    "    padding_length_output_seq = max_seq_length - len(output_seq)\n",
    "    if ( padding_length_output_seq > 0 ):\n",
    "        output_seq += [0] * padding_length_output_seq\n",
    "    '''\n",
    "    padding_length_input_seq = max_seq_length - len(input_seq)\n",
    "    if ( padding_length_input_seq > 0 ):\n",
    "        input_seq += [tokenizer.pad_token_id] * padding_length_input_seq\n",
    "    padding_length_output_seq = max_seq_length - len(output_seq)\n",
    "    if ( padding_length_output_seq > 0 ):\n",
    "        output_seq += [tokenizer.pad_token_id] * padding_length_output_seq\n",
    "    '''\n",
    "    # Add the encoded sequences to the input and output lists\n",
    "    input_ids.append(input_seq)\n",
    "    output_ids.append(output_seq)\n",
    "\n",
    "# Convert inputs and outputs to numpy arrays\n",
    "# also change the type from object to int64\n",
    "input_ids = np.array(input_ids).astype('int64') # Vectorized prompts\n",
    "output_ids = np.array(output_ids).astype('int64') # Vectorized target texts\n",
    "\n",
    "# Split data into training and validation sets\n",
    "split_length = int(len(input_ids) * 0.9)\n",
    "train_input_ids = input_ids[:split_length]\n",
    "train_output_ids = output_ids[:split_length]\n",
    "val_input_ids = input_ids[split_length:]\n",
    "val_output_ids = output_ids[split_length:]\n",
    "print(\"총 데이터셋 개수:\", len(input_ids))\n",
    "print(\"훈련 데이터셋 수:\", split_length)\n",
    "print(\"검증 데이터셋 수:\", len(input_ids) - split_length)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# Temporary setting to print full numpy array\n",
    "'''\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(train_input_ids.shape)\n",
    "    print(train_input_ids[:2])\n",
    "    print(train_output_ids.shape)\n",
    "    print(train_output_ids[:2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ebaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "print(\"[모델 학습 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
    "\n",
    "# 모델 학습 시 성능이 개선되지 않는 횟수가 2회를 초과하면 학습을 멈춤\n",
    "earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit( train_input_ids, train_output_ids,\n",
    "          validation_data=(val_input_ids,val_output_ids),\n",
    "          callbacks=[earlystop],\n",
    "          batch_size = 4, # 약 12GB 메모리사용\n",
    "          epochs = 10)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "# 모델 저장\n",
    "\n",
    "model.save_pretrained(\"output/finetuned-kogpt2-cs-comment-generation\")\n",
    "tokenizer.save_pretrained(\"output/finetuned-kogpt2-cs-comment-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation val_loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드 및 테스트\n",
    "\n",
    "print(\"[모델 로딩 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "loaded_model = TFGPT2LMHeadModel.from_pretrained(\"output/finetuned-kogpt2-cs-comment-generation\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"output/finetuned-kogpt2-cs-comment-generation\")\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print(\"[문장 생성 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "prompt = \"환불 요청 안함 | 탑승 전 | 엑셀,바퀴 | 엑셀이 작동을 안해요\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "output_ids = model.generate(input_ids=input_ids, max_length=280, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt: \" + prompt)\n",
    "print(\"Generated text: \" + generated_text)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551c7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDT(Current Date and Time): 2023-03-15 16:32:10.822238\n",
      "[모델 학습 시간]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbike/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377: FutureWarning: The old compute_loss method is deprecated as it conflicts with the Keras compute_loss method added in TF 2.8. If you want the original HF compute_loss, please call hf_compute_loss() instead. From TF versions >= 2.8, or Transformers versions >= 5, calling compute_loss() will get the Keras method instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26462/26462 [==============================] - 86261s 3s/step - loss: 0.5165 - val_loss: 0.4861\n",
      "Time duration(in seconds): 86261.62590811285\n",
      "CDT(Current Date and Time): 2023-03-16 16:29:52.448478\n",
      "[문장 생성 시간]\n",
      "Prompt: 환불 요청 안함 | 탑승 전 | 엑셀,바퀴 | 엑셀이 작동을 안해요\n",
      "Generated text: 환불 요청 안함 | 탑승 전 | 엑셀,바퀴 | 엑셀이 작동을 안해요 수거/점검 진행하도록 하겠습니다. 감사합니다. 또한,기는팀 점검하였습니다. 확인하여 이용에드 않도록하겠 감사 접수 꼼꼼히 확인 진행할 있도록습니다. 더욱 하겠 빠른릴 수 노력 운영립니다.합니다.습니다.합니다.합니다. 감사 감사 하겠 감사 수합니다. 노력 감사하겠 감사습니다.습니다.으로 불편 기 드 서비스 해당하여으로습니다. 접수합니다. 꼼꼼히리지시 부탁드리해 보려 반납겠며, 후 점 불편을 종료실 죄하다는 고객송 다시 번 환 양 답 이용어진될 신고금셔 도와하 미니 시 늦변일 경우구역 가능지면 참고 영업되 주차 기기 카드다 소요신 결제 확인 감사팀습니다. 확인 확인습니다. 수습니다.팀 감사 접수 접수 감사드습니다.드 감사 확인으로합니다. 수 감사 점검 감사 꼼꼼히 감사으로 감사하여 감사 운영 감사 노력합니다.팀팀합니다.으로 확인합니다. 확인\n",
      "Time duration(in seconds): 107.95419706590474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output/finetuned-kogpt2-cs-comment-generation-epoch1/tokenizer_config.json',\n",
       " 'output/finetuned-kogpt2-cs-comment-generation-epoch1/special_tokens_map.json',\n",
       " 'output/finetuned-kogpt2-cs-comment-generation-epoch1/vocab.json',\n",
       " 'output/finetuned-kogpt2-cs-comment-generation-epoch1/merges.txt',\n",
       " 'output/finetuned-kogpt2-cs-comment-generation-epoch1/added_tokens.json',\n",
       " 'output/finetuned-kogpt2-cs-comment-generation-epoch1/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에폭 한번 학습당 약 24시간(하루) 걸림 @ No GPU\n",
    "# Normal fine-tuning steps are around 10000s\n",
    "# and the number of epochs are around 10s\n",
    "print_current_datetime()\n",
    "\n",
    "print(\"[모델 학습 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
    "\n",
    "history = model.fit(train_input_ids, train_output_ids,\n",
    "          validation_data=(val_input_ids,val_output_ids),\n",
    "          batch_size = 4, # 약 12GB 메모리사용\n",
    "          epochs = 1)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "print(\"[문장 생성 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "# Generate text from prompt :\n",
    "# In text generation tasks, a 2-gram refers to a sequence of two consecutive words\n",
    "# and early_stopping parameter specifies whether to stop generation as soon as\n",
    "# all beam hypotheses (num_beams=5) have generated an end-of-sequence token\n",
    "prompt = \"환불 요청 안함 | 탑승 전 | 엑셀,바퀴 | 엑셀이 작동을 안해요\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "output_ids = model.generate(input_ids=input_ids, max_length=280, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt: \" + prompt)\n",
    "print(\"Generated text: \" + generated_text)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# 모델 저장\n",
    "\n",
    "model.save_pretrained(\"output/finetuned-kogpt2-cs-comment-generation-epoch1\")\n",
    "tokenizer.save_pretrained(\"output/finetuned-kogpt2-cs-comment-generation-epoch1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
