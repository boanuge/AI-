{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT : Please give me a good sample code for python keras model for classcification.\n",
    "\n",
    "'''\n",
    "Certainly! Here is an example of how you might use Keras, a high-level neural networks API, to create a multi-layer perceptron (MLP) model to classify a dataset of observations into different classes:\n",
    "\n",
    "I can provide you with sample code for creating X_train.npy and y_train.npy files, but I can't provide you the actual sample files, as these would depend on the specific dataset and problem you are working on.\n",
    "\n",
    "Here's an example of how you might create X_train.npy and y_train.npy files from a pandas dataframe :\n",
    "\n",
    "This code reads a CSV file containing training data into a pandas dataframe and it uses the drop function of dataframe to drop the label column from the dataframe and then convert dataframe to numpy array. Then it saves the arrays to .npy files so that you can use it in your code later. It also saves the label array as y_train.npy\n",
    "\n",
    "Keep in mind that this is a simple example, you may need to adjust the code based on the specifics of your dataset and file format.\n",
    "\n",
    "The predict method of a model returns an array of probabilities for each class, where the columns of the array correspond to the classes, and the rows correspond to the observations. predictions will have the shape (number of samples, number of classes).\n",
    "\n",
    "np.argmax(predictions, axis=1) function will return the indices of the maximum values along an axis. In this case, it returns the index of the maximum probability for each row, which corresponds to the class with the highest probability.\n",
    "\n",
    "So, these line of codes converts the predictions from an array of probability to an array of class labels, by taking the argmax of the predictions along the second axis (i.e. axis=1).\n",
    "You can then use these predictions to evaluate the model's performance using metrics such as accuracy, precision, recall, and F1 score.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72088cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT : Write the code for accuracy, precision, recall, and F1 score.\n",
    "\n",
    "'''\n",
    "Sure! Here is an example of how you might calculate the accuracy, precision, recall, and F1 score for a classification model's predictions in Python:\n",
    "\n",
    "Here, y_test is the ground truth labels of the test set, and predictions are the predictions of the model on the test set.\n",
    "\n",
    "accuracy_score, precision_score, recall_score, and f1_score are all functions from the sklearn.metrics module that can be used to calculate the accuracy, precision, recall, and F1 score, respectively. In the code, these are used to calculate the score of the model and store in the respective variables.\n",
    "\n",
    "The average parameter for precision_score, recall_score, and f1_score is set to 'weighted' that calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).\n",
    "\n",
    "You can then print the results to see the performance of the model on test set.\n",
    "\n",
    "Keep in mind that these metrics are not the only way to evaluate a model's performance, it depends on the problem and data you are working on. You may need to use\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4b97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read a CSV file into a pandas dataframe\n",
    "df1 = pd.read_csv(\"CLS_train_data.csv\")\n",
    "\n",
    "# Split the dataframe into feature and label arrays\n",
    "X_train = df1.drop(\"label\", axis=1).values\n",
    "y_train = df1[\"label\"].values\n",
    "\n",
    "# Save the arrays to .npy files\n",
    "np.save(\"CLS_X_train.npy\", X_train)\n",
    "np.save(\"CLS_y_train.npy\", y_train)\n",
    "\n",
    "# Read a CSV file into a pandas dataframe\n",
    "df2 = pd.read_csv(\"CLS_test_data.csv\")\n",
    "\n",
    "# Split the dataframe into feature and label arrays\n",
    "X_test = df2.drop(\"label\", axis=1).values\n",
    "y_test = df2[\"label\"].values\n",
    "\n",
    "# Save the arrays to .npy files\n",
    "np.save(\"CLS_X_test.npy\", X_test)\n",
    "np.save(\"CLS_y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdeff62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1500    3    1]\n",
      " [2400    4    2]\n",
      " [1800    3    3]\n",
      " [2200    4    2]\n",
      " [1700    3    1]]\n",
      "[1 2 1 3 1]\n",
      "[[2000    2    1]\n",
      " [1000    1    3]]\n",
      "[1 3]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 585.3041 - accuracy: 0.2000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 563.5609 - accuracy: 0.2000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 541.8474 - accuracy: 0.2000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 520.1725 - accuracy: 0.2000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 520.6025 - accuracy: 0.2000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 515.2533 - accuracy: 0.2000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 506.1921 - accuracy: 0.2000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 494.9304 - accuracy: 0.2000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 482.1598 - accuracy: 0.2000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 468.2748 - accuracy: 0.2000\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000201E7493CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[[0.000000e+00 0.000000e+00 5.709467e-09 1.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 6.343472e-05 9.999366e-01]]\n",
      "[3 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the training data\n",
    "X_train = np.load(\"CLS_X_train.npy\")\n",
    "y_train = np.load(\"CLS_y_train.npy\")\n",
    "\n",
    "# Load the test data\n",
    "X_test = np.load(\"CLS_X_test.npy\")\n",
    "y_test = np.load(\"CLS_y_test.npy\")\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Convert the predictions to the class with the highest probability\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8129476",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy of the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate the precision of the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and binary targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert the predictions to the class with the highest probability\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Calculate the precision of the model\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "\n",
    "# Calculate the recall of the model\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "# Calculate the F1 score of the model\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT : how to get accuracy, precision, recall, f1 score from classification model prediction\n",
    "'''\n",
    "In scikit-learn, the metrics module provides several functions for evaluating the performance of a classification model, including accuracy, precision, recall, and F1-score.\n",
    "\n",
    "Here's an example of how you can use the accuracy_score, precision_score, recall_score, and f1_score functions to calculate these metrics for a classification model:\n",
    "\n",
    "In this example, y_true is a list of true labels, and y_pred is a list of labels predicted by the classifier.\n",
    "\n",
    "If you are working with a multi-class problem, you can set the average parameter in the function to be either 'micro' or 'macro', to average the scores across all classes or to get scores separately for each class\n",
    "\n",
    "You can also use the function classification_report from sklearn.metrics which will give all of the above values and also other information like support for each class in a single function call.\n",
    "\n",
    "These metrics are a good starting point for evaluating the performance of a classification model, but it's important to keep in mind that they do have their limitations and should be used in conjunction with other evaluation metrics such as confusion matrix and ROC curves, or other measures specific to the problem you are working on.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92794833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8\n",
      "Precision:  1.0\n",
      "Recall:  0.6666666666666666\n",
      "F1-score:  0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.83      0.83      0.80         5\n",
      "weighted avg       0.87      0.80      0.80         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Get the true labels and predicted labels\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0]\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy: \", acc)\n",
    "\n",
    "# Calculate the precision\n",
    "prec = precision_score(y_true, y_pred)\n",
    "print(\"Precision: \", prec)\n",
    "\n",
    "# Calculate the recall\n",
    "rec = recall_score(y_true, y_pred)\n",
    "print(\"Recall: \", rec)\n",
    "\n",
    "# Calculate the F1-score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1-score: \", f1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
